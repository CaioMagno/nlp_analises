{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSAMENTO\n",
    "Esse arquivo define funções de extração de características. Preferẽncia para o uso de léxicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Léxico de Cláudia Freitas\n",
    "\n",
    "Uso do léxico encontrado <a href = http://www.scielo.br/pdf/rbla/v13n4/aop2813.pdf>neste artigo</a>.\n",
    "Peguei o léxico de substantivos desse artigo (por enquanto) e salvei num arquivo txt. É necessário uma expressão regular para extrair as palavras do arquivo. \n",
    "\n",
    "As palavras encontram-se na seguinte forma: [word];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textos carregados\n",
      "Léxico carregado\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from database_utils import DatabaseConnector, build_dataframe, normalize_text\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "def extract_words(filename):\n",
    "    import re\n",
    "                       \n",
    "    f = open(filename, 'r')\n",
    "    file_content = ''.join(f.readlines())\n",
    "    f.close()\n",
    "    \n",
    "    words = re.findall(r'\\[(\\w+)\\]', file_content)\n",
    "    return words\n",
    "\n",
    "def get_data_from_db(sentiment = None):\n",
    "    db_connector = DatabaseConnector('localhost', 'root', '12345', 'CORPUS_VIES')\n",
    "    if sentiment != None:\n",
    "        retrieved_data = db_connector.getDataBySentiment(sentiment)\n",
    "    else:\n",
    "        retrieved_data = db_connector.getDataTextAndLabel()\n",
    "\n",
    "    return retrieved_data\n",
    "\n",
    "\n",
    "# Recuperação dos textos - treinar o modelo somente com as classes POS e NEG\n",
    "all_data = get_data_from_db()\n",
    "all_data = all_data[(all_data[\"labels\"] == \"PO\") | (all_data[\"labels\"] == \"NG\")]\n",
    "print('Textos carregados')\n",
    "\n",
    "subs_pos = extract_words('/home/caiomagno/Documentos/Analise de sentimento - aplicacoes/Recursos/Claudia Freitas/subs_pos')\n",
    "subs_neg = extract_words('/home/caiomagno/Documentos/Analise de sentimento - aplicacoes/Recursos/Claudia Freitas/subs_neg')\n",
    "verbs_pos = extract_words('/home/caiomagno/Documentos/Analise de sentimento - aplicacoes/Recursos/Claudia Freitas/verbs_pos')\n",
    "verbs_neg = extract_words('/home/caiomagno/Documentos/Analise de sentimento - aplicacoes/Recursos/Claudia Freitas/verbs_neg')\n",
    "adj_pos = extract_words('/home/caiomagno/Documentos/Analise de sentimento - aplicacoes/Recursos/Claudia Freitas/adj_pos')\n",
    "adj_neg = extract_words('/home/caiomagno/Documentos/Analise de sentimento - aplicacoes/Recursos/Claudia Freitas/adj_neg')\n",
    "print('Léxico carregado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extração de features realizadas\n",
      "\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.732394366197\n",
      "Fold  1  - Acuracia:  0.757142857143\n",
      "Fold  2  - Acuracia:  0.8\n",
      "Fold  3  - Acuracia:  0.757142857143\n",
      "Fold  4  - Acuracia:  0.814285714286\n",
      "Fold  5  - Acuracia:  0.728571428571\n",
      "Fold  6  - Acuracia:  0.814285714286\n",
      "Fold  7  - Acuracia:  0.7\n",
      "Fold  8  - Acuracia:  0.657142857143\n",
      "Fold  9  - Acuracia:  0.628571428571\n",
      "\n",
      "Accuracia media:  0.738953722334\n",
      "Desvio padrão:  0.0602358974283\n"
     ]
    }
   ],
   "source": [
    "# Vetorizando os textos do corpus viés através do léxico de Cláudia Freitas\n",
    "lexicon = list(set(subs_neg + subs_pos + verbs_neg +verbs_pos + adj_neg + adj_pos))\n",
    "ftu = FeatureUnion([(\"unigram\", CountVectorizer()), (\"bigram\", CountVectorizer(ngram_range=(1,2))), (\"lexicon_vector\", CountVectorizer(vocabulary= lexicon))])\n",
    "print('Extração de features realizadas\\n')\n",
    "\n",
    "sKFold = StratifiedKFold(all_data['labels'], n_folds=10, shuffle=True)\n",
    "\n",
    "print(\"Cross Validation:\")\n",
    "accuracy_average = np.array([])\n",
    "for index, (train, test) in enumerate(sKFold):\n",
    "    # Treinando um modelo Naive Bayes\n",
    "    train_data = all_data.iloc[train]\n",
    "    test_data = all_data.iloc[test]\n",
    "    \n",
    "    p = Pipeline([(\"features\", ftu), (\"classifier\", MultinomialNB())])\n",
    "    p.fit(train_data[\"texts\"], train_data[\"labels\"])\n",
    "    predictions = p.predict(test_data[\"texts\"])\n",
    "\n",
    "    accuracy = accuracy_score(test_data[\"labels\"], predictions, normalize=True)\n",
    "    accuracy_average = np.append(accuracy_average, accuracy)\n",
    "    print(\"Fold \", index, \" - Acuracia: \", accuracy)\n",
    "    \n",
    "\n",
    "print(\"\\nAccuracia media: \", accuracy_average.mean())\n",
    "print(\"Desvio padrão: \", accuracy_average.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando a matriz de features com PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidade do problema\n",
      "Dimensão do léxico:  513\n",
      "Dimensões da matriz:  (2, 517)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "lexicon = list(set(subs_neg + subs_pos + verbs_neg +verbs_pos + adj_neg + adj_pos))\n",
    "ftu = FeatureUnion([(\"unigram\", CountVectorizer()), (\"bigram\", CountVectorizer(ngram_range=(1,2))), (\"lexicon_vector\", CountVectorizer(vocabulary= lexicon))])\n",
    "\n",
    "cv_unigram = CountVectorizer()\n",
    "cv_bigram = CountVectorizer()\n",
    "\n",
    "unigram = \n",
    "\n",
    "print(\"Dimensionalidade do problema\")\n",
    "print(\"Dimensão do léxico: \", len(lexicon))\n",
    "print(\"Dimensões da matriz: \", bag_of_features.shape)\n",
    "\n",
    "bag_of_"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
