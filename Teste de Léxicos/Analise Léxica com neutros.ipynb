{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "all_data = get_data_from_db()\n",
    "\n",
    "num_remover = NumRemover()\n",
    "word_remover = WordRemover(stopwords.words('portuguese'))\n",
    "all_data = num_remover.fit_transform(all_data)\n",
    "all_data = word_remover.fit_transform(all_data)\n",
    "\n",
    "neg_data = all_data[all_data[\"labels\"] == \"NG\"]\n",
    "neu_data = all_data[all_data[\"labels\"] == \"NE\"]\n",
    "pos_data = all_data[all_data[\"labels\"] == \"PO\"]\n",
    "\n",
    "ratio = 0.7\n",
    "neg_train = neg_data.iloc[0:round(ratio*neg_data.shape[0]), :]\n",
    "neu_train = neu_data.iloc[0:round(ratio*neu_data.shape[0]), :]\n",
    "pos_train = pos_data.iloc[0:round(ratio*pos_data.shape[0]), :]\n",
    "\n",
    "neg_test = neg_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "neu_test = neu_data.iloc[round(ratio*neu_data.shape[0]): , :]\n",
    "pos_test = pos_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "\n",
    "stopwords_pt = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão vocabulario negativo: 2687\n",
      "Dimensão vocabulario neutro: 2145\n",
      "Dimensão vocabulario positivo: 2326\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab_neg = set(' '.join(neg_train[\"texts\"].tolist()).split())\n",
    "vocab_neu = set(' '.join(neu_train[\"texts\"].tolist()).split())\n",
    "vocab_pos = set(' '.join(pos_train[\"texts\"].tolist()).split())\n",
    "\n",
    "print(\"Dimensão vocabulario negativo: \" + str(len(vocab_neg)))\n",
    "print(\"Dimensão vocabulario neutro: \" + str(len(vocab_neu)))\n",
    "print(\"Dimensão vocabulario positivo: \" + str(len(vocab_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Verificar interseção dos vocabulários\n",
    "intersect = vocab_neg.intersection(vocab_pos.intersection(vocab_neu))\n",
    "vocab_neg_excl = vocab_neg.difference(intersect)\n",
    "vocab_neu_excl = vocab_neu.difference(intersect)\n",
    "vocab_pos_excl = vocab_pos.difference(intersect)\n",
    "\n",
    "lexicon = list(vocab_neg.union(vocab_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.537735849057\n",
      "Fold  1  - Acuracia:  0.490384615385\n",
      "Fold  2  - Acuracia:  0.528846153846\n",
      "Fold  3  - Acuracia:  0.5\n",
      "Fold  4  - Acuracia:  0.509615384615\n",
      "Fold  5  - Acuracia:  0.576923076923\n",
      "Fold  6  - Acuracia:  0.596153846154\n",
      "Fold  7  - Acuracia:  0.586538461538\n",
      "Fold  8  - Acuracia:  0.586538461538\n",
      "Fold  9  - Acuracia:  0.548076923077\n",
      "\n",
      "Accuracia media:  0.546081277213\n",
      "Desvio padrão:  0.0369568506763\n",
      "\n",
      "\n",
      "SVM\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.584905660377\n",
      "Fold  1  - Acuracia:  0.528846153846\n",
      "Fold  2  - Acuracia:  0.442307692308\n",
      "Fold  3  - Acuracia:  0.480769230769\n",
      "Fold  4  - Acuracia:  0.576923076923\n",
      "Fold  5  - Acuracia:  0.548076923077\n",
      "Fold  6  - Acuracia:  0.576923076923\n",
      "Fold  7  - Acuracia:  0.557692307692\n",
      "Fold  8  - Acuracia:  0.586538461538\n",
      "Fold  9  - Acuracia:  0.528846153846\n",
      "\n",
      "Accuracia media:  0.54118287373\n",
      "Desvio padrão:  0.0453587081655\n",
      "\n",
      "Maxent\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.556603773585\n",
      "Fold  1  - Acuracia:  0.5\n",
      "Fold  2  - Acuracia:  0.538461538462\n",
      "Fold  3  - Acuracia:  0.490384615385\n",
      "Fold  4  - Acuracia:  0.557692307692\n",
      "Fold  5  - Acuracia:  0.538461538462\n",
      "Fold  6  - Acuracia:  0.567307692308\n",
      "Fold  7  - Acuracia:  0.519230769231\n",
      "Fold  8  - Acuracia:  0.567307692308\n",
      "Fold  9  - Acuracia:  0.528846153846\n",
      "\n",
      "Accuracia media:  0.536429608128\n",
      "Desvio padrão:  0.0256414421103\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = FeatureUnion([\n",
    "#                     (\"bigram\", CountVectorizer(ngram_range=(2,2), stop_words= stopwords_pt, binary= True)),\n",
    "                    (\"lexicon_vector\", CountVectorizer(vocabulary= lexicon, binary = True))\n",
    "                    ])\n",
    "\n",
    "folds = 10;\n",
    "print(\"MultinomialNB\")\n",
    "run_cross_validation(all_data, features, MultinomialNB(), n_folds= folds)\n",
    "\n",
    "print(\"\\n\\nSVM\")\n",
    "run_cross_validation(all_data, features, SVC(C=316), n_folds=folds)\n",
    "\n",
    "print(\"\\nMaxent\")\n",
    "run_cross_validation(all_data, features, LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False), n_folds=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Avaliando a incidência dos bigramas\n",
    "cv = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "neg_counts = cv.fit_transform(neg_train[\"texts\"])\n",
    "neg_bigrams = cv.vocabulary_\n",
    "\n",
    "pos_counts = cv.fit_transform(pos_train[\"texts\"])\n",
    "pos_bigrams = cv.vocabulary_\n",
    "\n",
    "neu_counts = cv.fit_transform(neu_train[\"texts\"])\n",
    "neu_bigrams = cv.vocabulary_\n",
    "\n",
    "spo = np.sum(pos_counts.toarray(), axis=0)\n",
    "sng = np.sum(neg_counts.toarray(), axis=0)\n",
    "sne = np.sum(neu_counts.toarray(), axis=0)\n",
    "\n",
    "pos_bigrams  = {bigram: spo[index] for bigram, index in pos_bigrams.items()}\n",
    "neg_bigrams  = {bigram: sng[index] for bigram, index in neg_bigrams.items()}\n",
    "neu_bigrams  = {bigram: sne[index] for bigram, index in neu_bigrams.items()}\n",
    "\n",
    "pos_excl_bigrams = {key:value for key, value in pos_bigrams.items() if key not in neg_bigrams and key not in neu_bigrams}\n",
    "neg_excl_bigrams = {key:value for key, value in neg_bigrams.items() if key not in pos_bigrams and key not in neu_bigrams}\n",
    "neu_excl_bigrams = {key:value for key, value in neu_bigrams.items() if key not in pos_bigrams and key not in neg_bigrams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "educação                    20\n",
       "saúde                       10\n",
       "saúde educação               6\n",
       "américa                      5\n",
       "américa latina               5\n",
       "cada vez                     5\n",
       "educação corrupção           5\n",
       "investir                     5\n",
       "latina                       5\n",
       "punidos                      5\n",
       "quatro anos                  5\n",
       "saúde educação corrupção     5\n",
       "cidade                       4\n",
       "drogas                       4\n",
       "educação saúde               4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Series\n",
    "s = Series(pos_excl_bigrams)\n",
    "s.nlargest(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.283018867925\n",
      "Fold  1  - Acuracia:  0.163461538462\n",
      "Fold  2  - Acuracia:  0.278846153846\n",
      "Fold  3  - Acuracia:  0.153846153846\n",
      "Fold  4  - Acuracia:  0.298076923077\n",
      "Fold  5  - Acuracia:  0.298076923077\n",
      "Fold  6  - Acuracia:  0.269230769231\n",
      "Fold  7  - Acuracia:  0.230769230769\n",
      "Fold  8  - Acuracia:  0.298076923077\n",
      "Fold  9  - Acuracia:  0.163461538462\n",
      "\n",
      "Accuracia media:  0.243686502177\n",
      "Desvio padrão:  0.0578108407068\n",
      "\n",
      "\n",
      "SVM\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.405660377358\n",
      "Fold  1  - Acuracia:  0.625\n",
      "Fold  2  - Acuracia:  0.394230769231\n",
      "Fold  3  - Acuracia:  0.605769230769\n",
      "Fold  4  - Acuracia:  0.644230769231\n",
      "Fold  5  - Acuracia:  0.625\n",
      "Fold  6  - Acuracia:  0.673076923077\n",
      "Fold  7  - Acuracia:  0.615384615385\n",
      "Fold  8  - Acuracia:  0.384615384615\n",
      "Fold  9  - Acuracia:  0.605769230769\n",
      "\n",
      "Accuracia media:  0.557873730044\n",
      "Desvio padrão:  0.108444927807\n",
      "\n",
      "Maxent\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.707547169811\n",
      "Fold  1  - Acuracia:  0.730769230769\n",
      "Fold  2  - Acuracia:  0.759615384615\n",
      "Fold  3  - Acuracia:  0.75\n",
      "Fold  4  - Acuracia:  0.759615384615\n",
      "Fold  5  - Acuracia:  0.759615384615\n",
      "Fold  6  - Acuracia:  0.778846153846\n",
      "Fold  7  - Acuracia:  0.740384615385\n",
      "Fold  8  - Acuracia:  0.730769230769\n",
      "Fold  9  - Acuracia:  0.692307692308\n",
      "\n",
      "Accuracia media:  0.740947024673\n",
      "Desvio padrão:  0.0250052986037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "lexicon_bigrams = list(pos_excl_bigrams.keys()) + list(neg_excl_bigrams.keys())\n",
    "\n",
    "features = FeatureUnion([\n",
    "#                     (\"bigram\", CountVectorizer(ngram_range=(1,1), stop_words= stopwords_pt, vocabulary  = lexicon)),\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_bigrams))\n",
    "                    ])\n",
    "\n",
    "folds = 10;\n",
    "print(\"MultinomialNB\")\n",
    "run_cross_validation(all_data, features, MultinomialNB(), n_folds= folds)\n",
    "\n",
    "print(\"\\n\\nSVM\")\n",
    "run_cross_validation(all_data, features, SVC(C=316), n_folds=folds)\n",
    "\n",
    "print(\"\\nMaxent\")\n",
    "run_cross_validation(all_data, features, LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False), n_folds=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
