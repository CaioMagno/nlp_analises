{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "all_data = get_data_from_db()\n",
    "\n",
    "num_remover = NumRemover()\n",
    "word_remover = WordRemover(stopwords.words('portuguese'))\n",
    "all_data = num_remover.fit_transform(all_data)\n",
    "all_data = word_remover.fit_transform(all_data)\n",
    "\n",
    "neg_data = all_data[all_data[\"labels\"] == \"NG\"]\n",
    "neu_data = all_data[all_data[\"labels\"] == \"NE\"]\n",
    "pos_data = all_data[all_data[\"labels\"] == \"PO\"]\n",
    "\n",
    "ratio = 0.7\n",
    "neg_train = neg_data.iloc[0:round(ratio*neg_data.shape[0]), :]\n",
    "neu_train = neu_data.iloc[0:round(ratio*neu_data.shape[0]), :]\n",
    "pos_train = pos_data.iloc[0:round(ratio*pos_data.shape[0]), :]\n",
    "\n",
    "neg_test = neg_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "neu_test = neu_data.iloc[round(ratio*neu_data.shape[0]): , :]\n",
    "pos_test = pos_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "\n",
    "stopwords_pt = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão vocabulario negativo: 2692\n",
      "Dimensão vocabulario neutro: 2153\n",
      "Dimensão vocabulario positivo: 2330\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab_neg = set(' '.join(neg_train[\"texts\"].tolist()).split())\n",
    "vocab_neu = set(' '.join(neu_train[\"texts\"].tolist()).split())\n",
    "vocab_pos = set(' '.join(pos_train[\"texts\"].tolist()).split())\n",
    "\n",
    "print(\"Dimensão vocabulario negativo: \" + str(len(vocab_neg)))\n",
    "print(\"Dimensão vocabulario neutro: \" + str(len(vocab_neu)))\n",
    "print(\"Dimensão vocabulario positivo: \" + str(len(vocab_pos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removendo somente a interseção central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Verificar interseção dos vocabulários\n",
    "intersect = vocab_neg.intersection(vocab_pos.intersection(vocab_neu))\n",
    "vocab_neg_excl = vocab_neg.difference(intersect)\n",
    "vocab_neu_excl = vocab_neu.difference(intersect)\n",
    "vocab_pos_excl = vocab_pos.difference(intersect)\n",
    "\n",
    "lexicon = list(vocab_neg.union(vocab_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.537735849057\n",
      "Fold  1  - Acuracia:  0.490384615385\n",
      "Fold  2  - Acuracia:  0.528846153846\n",
      "Fold  3  - Acuracia:  0.5\n",
      "Fold  4  - Acuracia:  0.509615384615\n",
      "Fold  5  - Acuracia:  0.576923076923\n",
      "Fold  6  - Acuracia:  0.596153846154\n",
      "Fold  7  - Acuracia:  0.586538461538\n",
      "Fold  8  - Acuracia:  0.586538461538\n",
      "Fold  9  - Acuracia:  0.548076923077\n",
      "\n",
      "Accuracia media:  0.546081277213\n",
      "Desvio padrão:  0.0369568506763\n",
      "\n",
      "\n",
      "SVM\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.584905660377\n",
      "Fold  1  - Acuracia:  0.528846153846\n",
      "Fold  2  - Acuracia:  0.442307692308\n",
      "Fold  3  - Acuracia:  0.480769230769\n",
      "Fold  4  - Acuracia:  0.576923076923\n",
      "Fold  5  - Acuracia:  0.548076923077\n",
      "Fold  6  - Acuracia:  0.576923076923\n",
      "Fold  7  - Acuracia:  0.557692307692\n",
      "Fold  8  - Acuracia:  0.586538461538\n",
      "Fold  9  - Acuracia:  0.528846153846\n",
      "\n",
      "Accuracia media:  0.54118287373\n",
      "Desvio padrão:  0.0453587081655\n",
      "\n",
      "Maxent\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.556603773585\n",
      "Fold  1  - Acuracia:  0.5\n",
      "Fold  2  - Acuracia:  0.538461538462\n",
      "Fold  3  - Acuracia:  0.490384615385\n",
      "Fold  4  - Acuracia:  0.557692307692\n",
      "Fold  5  - Acuracia:  0.538461538462\n",
      "Fold  6  - Acuracia:  0.567307692308\n",
      "Fold  7  - Acuracia:  0.519230769231\n",
      "Fold  8  - Acuracia:  0.567307692308\n",
      "Fold  9  - Acuracia:  0.528846153846\n",
      "\n",
      "Accuracia media:  0.536429608128\n",
      "Desvio padrão:  0.0256414421103\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(vocabulary= lexicon, binary = True))\n",
    "                    ])\n",
    "\n",
    "folds = 10;\n",
    "print(\"MultinomialNB\")\n",
    "run_cross_validation(all_data, features, MultinomialNB(), n_folds= folds)\n",
    "\n",
    "print(\"\\n\\nSVM\")\n",
    "run_cross_validation(all_data, features, SVC(C=316), n_folds=folds)\n",
    "\n",
    "print(\"\\nMaxent\")\n",
    "run_cross_validation(all_data, features, LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False), n_folds=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ngram_lexicon(ngram_range):\n",
    "    # Avaliando a incidência dos bigramas\n",
    "    cv = CountVectorizer(ngram_range=ngram_range, stop_words = stopwords_pt)\n",
    "\n",
    "    neg_counts = cv.fit_transform(neg_train[\"texts\"])\n",
    "    neg_ngrams = set(cv.vocabulary_)\n",
    "\n",
    "    pos_counts = cv.fit_transform(pos_train[\"texts\"])\n",
    "    pos_ngrams = set(cv.vocabulary_)\n",
    "\n",
    "    neu_counts = cv.fit_transform(neu_train[\"texts\"])\n",
    "    neu_ngrams = set(cv.vocabulary_)\n",
    "\n",
    "    # Encontrando as interseções\n",
    "    central_intersection = pos_ngrams.intersection(neg_ngrams).intersection(neu_ngrams)\n",
    "    pos_neg_intersection = pos_ngrams.intersection(neg_ngrams)\n",
    "    pos_neu_intersection = pos_ngrams.intersection(neu_ngrams)\n",
    "    neu_neg_intersection = neu_ngrams.intersection(neg_ngrams)\n",
    "\n",
    "    total_intersection = central_intersection.union(pos_neg_intersection).union(pos_neu_intersection).union(neu_neg_intersection)\n",
    "\n",
    "    #Removendo as interseções do vocabulario\n",
    "    lexicon_ngrams = neg_ngrams.union(pos_ngrams).union(neu_ngrams).difference(total_intersection)\n",
    "\n",
    "    print(\"Dimensionalidade: \", len(lexicon_ngrams))\n",
    "    return lexicon_ngrams\n",
    "\n",
    "# neg_counts = cv.fit_transform(neg_train[\"texts\"])\n",
    "# neg_ngrams = cv.vocabulary_\n",
    "\n",
    "# pos_counts = cv.fit_transform(pos_train[\"texts\"])\n",
    "# pos_ngrams = cv.vocabulary_\n",
    "\n",
    "# neu_counts = cv.fit_transform(neu_train[\"texts\"])\n",
    "# neu_ngrams = cv.vocabulary_\n",
    "\n",
    "# spo = np.sum(pos_counts.toarray(), axis=0)\n",
    "# sng = np.sum(neg_counts.toarray(), axis=0)\n",
    "# sne = np.sum(neu_counts.toarray(), axis=0)\n",
    "\n",
    "# pos_ngrams  = {ngram: spo[index] for ngram, index in pos_ngrams.items()}\n",
    "# neg_ngrams  = {ngram: sng[index] for ngram, index in neg_ngrams.items()}\n",
    "# neu_ngrams  = {ngram: sne[index] for ngram, index in neu_ngrams.items()}\n",
    "\n",
    "# pos_excl_ngrams = {key:value for key, value in pos_ngrams.items() if key not in neg_ngrams and key not in neu_ngrams}\n",
    "# neg_excl_ngrams = {key:value for key, value in neg_ngrams.items() if key not in pos_ngrams and key not in neu_ngrams}\n",
    "# neu_excl_ngrams = {key:value for key, value in neu_ngrams.items() if key not in pos_ngrams and key not in neg_ngrams}\n",
    "\n",
    "# lexicon_ngrams = list(pos_excl_ngrams.keys()) + list(neg_excl_ngrams.keys()) + list(neu_excl_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edison               12\n",
       "edison lobão         12\n",
       "sabesp               11\n",
       "calheiros pmdb        8\n",
       "lobão minas           8\n",
       "ministro edison       8\n",
       "presidentes           8\n",
       "beneficiários         7\n",
       "collor                7\n",
       "mário                 7\n",
       "mário negromonte      7\n",
       "acabou                6\n",
       "governadora           6\n",
       "jânio                 6\n",
       "líderes congresso     6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Series\n",
    "s = Series(neg_excl_ngrams)\n",
    "s.nlargest(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Somente com Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidade:  2913\n",
      "UNIGRAMS FREQUENCY**************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.622949927431\n",
      "Desvio padrão:  0.0441834796332\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.643033381713\n",
      "Desvio padrão:  0.030991717085\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.58448838897\n",
      "Desvio padrão:  0.0442820742617\n",
      "\n",
      "\n",
      "\n",
      "UNIGRAMS PRESENCE**************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.62198838897\n",
      "Desvio padrão:  0.0438129682017\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.651687227866\n",
      "Desvio padrão:  0.0308633974152\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.583508708273\n",
      "Desvio padrão:  0.0492484030048\n"
     ]
    }
   ],
   "source": [
    "lexicon_ngrams = get_ngram_lexicon(ngram_range=(1,1))\n",
    "\n",
    "print(\"UNIGRAMS FREQUENCY**************************************\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,1), stop_words=stopwords_pt, vocabulary= lexicon_ngrams))\n",
    "                    ])\n",
    "evaluate(all_data, features, n_folds= 10)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"UNIGRAMS PRESENCE**************************************\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,1), stop_words=stopwords_pt, vocabulary= lexicon_ngrams, binary= True))\n",
    "                    ])\n",
    "evaluate(all_data, features, n_folds= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Somente com Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidade:  13332\n",
      "BIGRAMS FREQUENCY**************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.591055878084\n",
      "Desvio padrão:  0.0485226924291\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.645881712627\n",
      "Desvio padrão:  0.0341021374824\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.48084179971\n",
      "Desvio padrão:  0.0294336393163\n",
      "\n",
      "\n",
      "\n",
      "BIGRAMS PRESENCE**************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.590076197388\n",
      "Desvio padrão:  0.0531563928766\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.645863570392\n",
      "Desvio padrão:  0.0411555314181\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.48084179971\n",
      "Desvio padrão:  0.0335442224988\n"
     ]
    }
   ],
   "source": [
    "lexicon_ngrams = get_ngram_lexicon(ngram_range=(2,2))\n",
    "\n",
    "print(\"BIGRAMS FREQUENCY**************************************\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(2,2), stop_words=stopwords_pt, vocabulary= lexicon_ngrams))\n",
    "                    ])\n",
    "evaluate(all_data, features, n_folds= 10)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"BIGRAMS PRESENCE**************************************\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(2,2), stop_words=stopwords_pt, vocabulary= lexicon_ngrams, binary= True))\n",
    "                    ])\n",
    "evaluate(all_data, features, n_folds= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams + Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidade:  16245\n",
      "UNIGRAMS + BIGRAMS FREQUENCY**************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.658291001451\n",
      "Desvio padrão:  0.0243226756047\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.702539912917\n",
      "Desvio padrão:  0.0333809435451\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.568232946299\n",
      "Desvio padrão:  0.0326479035624\n",
      "\n",
      "\n",
      "\n",
      "UNIGRAMS + BIGRAMS PRESENCE**************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.663098693759\n",
      "Desvio padrão:  0.0285017920489\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.703465166909\n",
      "Desvio padrão:  0.0231368985063\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.563425253991\n",
      "Desvio padrão:  0.0334123825789\n"
     ]
    }
   ],
   "source": [
    "lexicon_ngrams = get_ngram_lexicon(ngram_range=(1,2))\n",
    "\n",
    "print(\"UNIGRAMS + BIGRAMS FREQUENCY**************************************\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_ngrams))\n",
    "                    ])\n",
    "evaluate(all_data, features, n_folds= 10)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"UNIGRAMS + BIGRAMS PRESENCE**************************************\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_ngrams, binary= True))\n",
    "                    ])\n",
    "evaluate(all_data, features, n_folds= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando LSA e MaxEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation:\n",
      "Accuracia media:  0.420301161103\n",
      "Desvio padrão:  0.0325551831623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pca = PCA()\n",
    "# pca = TruncatedSVD(n_components=500)\n",
    "cv  = CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_ngrams)\n",
    "\n",
    "t_data = cv.fit_transform(all_data[\"texts\"])\n",
    "t_data = pca.fit_transform(t_data.toarray())\n",
    "\n",
    "labels = np.array([])\n",
    "for label in all_data[\"labels\"]:\n",
    "    if label == 'PO':\n",
    "        labels = np.append(labels, [1])\n",
    "    elif label == \"NG\":\n",
    "        labels = np.append(labels, [-1])\n",
    "    elif label == \"NE\":\n",
    "        labels = np.append(labels, [0])\n",
    "\n",
    "labels.shape\n",
    "\n",
    "classifier = LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False)\n",
    "# classifier = MultinomialNB()\n",
    "\n",
    "print(\"Cross Validation:\")\n",
    "accuracy_average = np.array([])\n",
    "sKFold = StratifiedKFold(n_splits= 10, shuffle= True, random_state= True)\n",
    "for index, (train, test) in enumerate(sKFold.split(t_data, labels)):\n",
    "    # Treinando um modelo Naive Bayes\n",
    "    train_data = t_data[train]\n",
    "    test_data = t_data[test]\n",
    "\n",
    "    classifier.fit(train_data, labels[train])\n",
    "    predictions = classifier.predict(test_data)\n",
    "\n",
    "    accuracy = accuracy_score(labels[test], predictions, normalize=True)\n",
    "    # classifier_models.append(pipeline)\n",
    "\n",
    "    accuracy_average = np.append(accuracy_average, accuracy)\n",
    "    #print(\"Fold \", index, \" - Acuracia: \", accuracy)\n",
    "\n",
    "print(\"Accuracia media: \", accuracy_average.mean())\n",
    "print(\"Desvio padrão: \", accuracy_average.std())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
