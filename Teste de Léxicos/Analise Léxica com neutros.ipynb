{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "all_data = get_data_from_db()\n",
    "\n",
    "num_remover = NumRemover()\n",
    "word_remover = WordRemover(stopwords.words('portuguese'))\n",
    "all_data = num_remover.fit_transform(all_data)\n",
    "all_data = word_remover.fit_transform(all_data)\n",
    "\n",
    "neg_data = all_data[all_data[\"labels\"] == \"NG\"]\n",
    "neu_data = all_data[all_data[\"labels\"] == \"NE\"]\n",
    "pos_data = all_data[all_data[\"labels\"] == \"PO\"]\n",
    "\n",
    "ratio = 0.4\n",
    "neg_train = neg_data.iloc[0:round(ratio*neg_data.shape[0]), :]\n",
    "neu_train = neu_data.iloc[0:round(ratio*neu_data.shape[0]), :]\n",
    "pos_train = pos_data.iloc[0:round(ratio*pos_data.shape[0]), :]\n",
    "\n",
    "neg_test = neg_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "neu_test = neu_data.iloc[round(ratio*neu_data.shape[0]): , :]\n",
    "pos_test = pos_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "\n",
    "stopwords_pt = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão vocabulario negativo: 3485\n",
      "Dimensão vocabulario neutro: 2759\n",
      "Dimensão vocabulario positivo: 2973\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab_neg = set(' '.join(neg_train[\"texts\"].tolist()).split())\n",
    "vocab_neu = set(' '.join(neu_train[\"texts\"].tolist()).split())\n",
    "vocab_pos = set(' '.join(pos_train[\"texts\"].tolist()).split())\n",
    "\n",
    "print(\"Dimensão vocabulario negativo: \" + str(len(vocab_neg)))\n",
    "print(\"Dimensão vocabulario neutro: \" + str(len(vocab_neu)))\n",
    "print(\"Dimensão vocabulario positivo: \" + str(len(vocab_pos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removendo somente a interseção central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Verificar interseção dos vocabulários\n",
    "intersect = vocab_neg.intersection(vocab_pos.intersection(vocab_neu))\n",
    "vocab_neg_excl = vocab_neg.difference(intersect)\n",
    "vocab_neu_excl = vocab_neu.difference(intersect)\n",
    "vocab_pos_excl = vocab_pos.difference(intersect)\n",
    "\n",
    "lexicon = list(vocab_neg.union(vocab_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "Cross Validation:\n",
      "Accuracia media:  0.541346153846\n",
      "Desvio padrão:  0.0391878536006\n",
      "\n",
      "\n",
      "SVM\n",
      "Cross Validation:\n",
      "Accuracia media:  0.552703193033\n",
      "Desvio padrão:  0.0469909981568\n",
      "\n",
      "Maxent\n",
      "Cross Validation:\n",
      "Accuracia media:  0.570881712627\n",
      "Desvio padrão:  0.036761742242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(vocabulary= lexicon, binary = True))\n",
    "                    ])\n",
    "\n",
    "folds = 10;\n",
    "print(\"MultinomialNB\")\n",
    "run_cross_validation(all_data, features, MultinomialNB(), n_folds= folds)\n",
    "\n",
    "print(\"\\n\\nSVM\")\n",
    "run_cross_validation(all_data, features, SVC(C=316), n_folds=folds)\n",
    "\n",
    "print(\"\\nMaxent\")\n",
    "run_cross_validation(all_data, features, LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False), n_folds=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ngram_lexicon(ngram_range):\n",
    "    # Avaliando a incidência dos bigramas\n",
    "    cv = CountVectorizer(ngram_range=ngram_range, stop_words = stopwords_pt)\n",
    "\n",
    "    neg_counts = cv.fit_transform(neg_train[\"texts\"])\n",
    "    neg_ngrams = set(cv.vocabulary_)\n",
    "\n",
    "    pos_counts = cv.fit_transform(pos_train[\"texts\"])\n",
    "    pos_ngrams = set(cv.vocabulary_)\n",
    "\n",
    "    neu_counts = cv.fit_transform(neu_train[\"texts\"])\n",
    "    neu_ngrams = set(cv.vocabulary_)\n",
    "\n",
    "    # Encontrando as interseções\n",
    "    central_intersection = pos_ngrams.intersection(neg_ngrams).intersection(neu_ngrams)\n",
    "    pos_neg_intersection = pos_ngrams.intersection(neg_ngrams)\n",
    "    pos_neu_intersection = pos_ngrams.intersection(neu_ngrams)\n",
    "    neu_neg_intersection = neu_ngrams.intersection(neg_ngrams)\n",
    "\n",
    "    total_intersection = central_intersection.union(pos_neg_intersection).union(pos_neu_intersection).union(neu_neg_intersection)\n",
    "\n",
    "    #Removendo as interseções do vocabulario\n",
    "    lexicon_ngrams = neg_ngrams.union(pos_ngrams).union(neu_ngrams).difference(total_intersection)\n",
    "\n",
    "    print(\"Dimensionalidade: \", len(lexicon_ngrams))\n",
    "    return lexicon_ngrams\n",
    "\n",
    "# neg_counts = cv.fit_transform(neg_train[\"texts\"])\n",
    "# neg_ngrams = cv.vocabulary_\n",
    "\n",
    "# pos_counts = cv.fit_transform(pos_train[\"texts\"])\n",
    "# pos_ngrams = cv.vocabulary_\n",
    "\n",
    "# neu_counts = cv.fit_transform(neu_train[\"texts\"])\n",
    "# neu_ngrams = cv.vocabulary_\n",
    "\n",
    "# spo = np.sum(pos_counts.toarray(), axis=0)\n",
    "# sng = np.sum(neg_counts.toarray(), axis=0)\n",
    "# sne = np.sum(neu_counts.toarray(), axis=0)\n",
    "\n",
    "# pos_ngrams  = {ngram: spo[index] for ngram, index in pos_ngrams.items()}\n",
    "# neg_ngrams  = {ngram: sng[index] for ngram, index in neg_ngrams.items()}\n",
    "# neu_ngrams  = {ngram: sne[index] for ngram, index in neu_ngrams.items()}\n",
    "\n",
    "# pos_excl_ngrams = {key:value for key, value in pos_ngrams.items() if key not in neg_ngrams and key not in neu_ngrams}\n",
    "# neg_excl_ngrams = {key:value for key, value in neg_ngrams.items() if key not in pos_ngrams and key not in neu_ngrams}\n",
    "# neu_excl_ngrams = {key:value for key, value in neu_ngrams.items() if key not in pos_ngrams and key not in neg_ngrams}\n",
    "\n",
    "# lexicon_ngrams = list(pos_excl_ngrams.keys()) + list(neg_excl_ngrams.keys()) + list(neu_excl_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neg_excl_ngrams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8fe165cf119b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_excl_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'neg_excl_ngrams' is not defined"
     ]
    }
   ],
   "source": [
    "from pandas import Series\n",
    "s = Series(neg_excl_ngrams)\n",
    "s.nlargest(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Somente com Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidade:  3454\n",
      "UNIGRAMS FREQUENCY**************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.687137155298\n",
      "Desvio padrão:  0.0562521203575\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.71692670537\n",
      "Desvio padrão:  0.0542063178274\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.620917997097\n",
      "Desvio padrão:  0.0526882649495\n",
      "\n",
      "\n",
      "\n",
      "UNIGRAMS PRESENCE**************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.686175616836\n",
      "Desvio padrão:  0.0482760615473\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.71692670537\n",
      "Desvio padrão:  0.0521193956343\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.627648766328\n",
      "Desvio padrão:  0.056747171455\n"
     ]
    }
   ],
   "source": [
    "lexicon_ngrams = get_ngram_lexicon(ngram_range=(1,1))\n",
    "\n",
    "print(\"UNIGRAMS FREQUENCY**************************************\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,1), stop_words=stopwords_pt, vocabulary= lexicon_ngrams))\n",
    "                    ])\n",
    "evaluate(all_data, features, n_folds= 10)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"UNIGRAMS PRESENCE**************************************\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,1), stop_words=stopwords_pt, vocabulary= lexicon_ngrams, binary= True))\n",
    "                    ])\n",
    "evaluate(all_data, features, n_folds= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3454"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Somente com Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidade:  17501\n",
      "\n",
      " Dimensionalidade:  17501\n",
      "BIGRAMS FREQUENCY**************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.616092162554\n",
      "Desvio padrão:  0.0412408122012\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.73898766328\n",
      "Desvio padrão:  0.0393995760161\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.443468795356\n",
      "Desvio padrão:  0.0259476079279\n",
      "\n",
      "\n",
      "\n",
      "BIGRAMS PRESENCE**************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.616092162554\n",
      "Desvio padrão:  0.043211395596\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.73898766328\n",
      "Desvio padrão:  0.0393995760161\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.440602322206\n",
      "Desvio padrão:  0.026336277332\n"
     ]
    }
   ],
   "source": [
    "lexicon_ngrams = get_ngram_lexicon(ngram_range=(2,2))\n",
    "print(\"\\n Dimensionalidade: \", len(lexicon_ngrams))\n",
    "\n",
    "print(\"BIGRAMS FREQUENCY**************************************\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(2,2), stop_words=stopwords_pt, vocabulary= lexicon_ngrams))\n",
    "                    ])\n",
    "evaluate(all_data, features, n_folds= 10)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"BIGRAMS PRESENCE**************************************\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(2,2), stop_words=stopwords_pt, vocabulary= lexicon_ngrams, binary= True))\n",
    "                    ])\n",
    "evaluate(all_data, features, n_folds= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams + Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidade:  9980\n",
      "UNIGRAMS + BIGRAMS FREQUENCY**************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.589277939042\n",
      "Desvio padrão:  0.0483179495124\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.57578011611\n",
      "Desvio padrão:  0.0496045369628\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.522115384615\n",
      "Desvio padrão:  0.0354728206648\n",
      "\n",
      "\n",
      "\n",
      "UNIGRAMS + BIGRAMS PRESENCE**************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.5912191582\n",
      "Desvio padrão:  0.0481898747099\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.589169085631\n",
      "Desvio padrão:  0.055376583559\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.525925253991\n",
      "Desvio padrão:  0.0359972714007\n"
     ]
    }
   ],
   "source": [
    "lexicon_ngrams = get_ngram_lexicon(ngram_range=(1,2))\n",
    "\n",
    "print(\"UNIGRAMS + BIGRAMS FREQUENCY**************************************\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_ngrams))\n",
    "                    ])\n",
    "evaluate(all_data, features, n_folds= 10)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"UNIGRAMS + BIGRAMS PRESENCE**************************************\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_ngrams, binary= True))\n",
    "                    ])\n",
    "evaluate(all_data, features, n_folds= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando LSA e MaxEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation:\n",
      "Accuracia media:  0.416509433962\n",
      "Desvio padrão:  0.0490313027867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pca = PCA()\n",
    "# pca = TruncatedSVD(n_components=500)\n",
    "cv  = CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_ngrams)\n",
    "\n",
    "t_data = cv.fit_transform(all_data[\"texts\"])\n",
    "t_data = pca.fit_transform(t_data.toarray())\n",
    "\n",
    "labels = np.array([])\n",
    "for label in all_data[\"labels\"]:\n",
    "    if label == 'PO':\n",
    "        labels = np.append(labels, [1])\n",
    "    elif label == \"NG\":\n",
    "        labels = np.append(labels, [-1])\n",
    "    elif label == \"NE\":\n",
    "        labels = np.append(labels, [0])\n",
    "\n",
    "labels.shape\n",
    "\n",
    "classifier = LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False)\n",
    "# classifier = MultinomialNB()\n",
    "\n",
    "print(\"Cross Validation:\")\n",
    "accuracy_average = np.array([])\n",
    "sKFold = StratifiedKFold(n_splits= 10, shuffle= True, random_state= True)\n",
    "for index, (train, test) in enumerate(sKFold.split(t_data, labels)):\n",
    "    # Treinando um modelo Naive Bayes\n",
    "    train_data = t_data[train]\n",
    "    test_data = t_data[test]\n",
    "\n",
    "    classifier.fit(train_data, labels[train])\n",
    "    predictions = classifier.predict(test_data)\n",
    "\n",
    "    accuracy = accuracy_score(labels[test], predictions, normalize=True)\n",
    "    # classifier_models.append(pipeline)\n",
    "\n",
    "    accuracy_average = np.append(accuracy_average, accuracy)\n",
    "    #print(\"Fold \", index, \" - Acuracia: \", accuracy)\n",
    "\n",
    "print(\"Accuracia media: \", accuracy_average.mean())\n",
    "print(\"Desvio padrão: \", accuracy_average.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
