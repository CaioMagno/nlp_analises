{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "all_data = get_data_from_db()\n",
    "all_data = all_data[(all_data[\"labels\"]== \"PO\") | (all_data[\"labels\"]== \"NG\")]\n",
    "\n",
    "num_remover = NumRemover()\n",
    "word_remover = WordRemover(stopwords.words('portuguese'))\n",
    "all_data = num_remover.fit_transform(all_data)\n",
    "all_data = word_remover.fit_transform(all_data)\n",
    "\n",
    "neg_data = all_data[all_data[\"labels\"] == \"NG\"]\n",
    "pos_data = all_data[all_data[\"labels\"] == \"PO\"]\n",
    "\n",
    "ratio = 0.7\n",
    "neg_train = neg_data.iloc[0:round(ratio*neg_data.shape[0]), :]\n",
    "pos_train = pos_data.iloc[0:round(ratio*pos_data.shape[0]), :]\n",
    "\n",
    "neg_test = neg_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "pos_test = pos_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "\n",
    "stopwords_pt = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão vocabulario negativo: 3614\n",
      "Dimensão vocabulario positivo: 3044\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab_neg = set(' '.join(neg_train[\"texts\"].tolist()).split())\n",
    "vocab_pos = set(' '.join(pos_train[\"texts\"].tolist()).split())\n",
    "\n",
    "print(\"Dimensão vocabulario negativo: \" + str(len(vocab_neg)))\n",
    "print(\"Dimensão vocabulario positivo: \" + str(len(vocab_pos)))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 3,
>>>>>>> f51ec6b06cf723457f7c3dba65582cec84adc060
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão vocabulario negativo: 3614\n",
      "Dimensão vocabulario positivo: 3044\n",
      "Dimensão inters: 1499\n",
      "Dimensão vocabulario negativo: 2115\n",
      "Dimensão vocabulario positivo: 1545\n"
     ]
    }
   ],
   "source": [
    "# Verificar interseção dos vocabulários\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab_neg = set(' '.join(neg_train[\"texts\"].tolist()).split())\n",
    "vocab_pos = set(' '.join(pos_train[\"texts\"].tolist()).split())\n",
    "\n",
    "print(\"Dimensão vocabulario negativo: \" + str(len(vocab_neg)))\n",
    "print(\"Dimensão vocabulario positivo: \" + str(len(vocab_pos)))\n",
    "\n",
    "\n",
=======
   "outputs": [],
   "source": [
    "# Verificar interseção dos vocabulários\n",
>>>>>>> f51ec6b06cf723457f7c3dba65582cec84adc060
    "intersect = vocab_neg.intersection(vocab_pos)\n",
    "vocab_neg = vocab_neg.difference(intersect)\n",
    "vocab_pos = vocab_pos.difference(intersect)\n",
    "\n",
<<<<<<< HEAD
    "print(\"Dimensão inters: \" + str(len(intersect)))\n",
    "\n",
    "print(\"Dimensão vocabulario negativo: \" + str(len(vocab_neg)))\n",
    "print(\"Dimensão vocabulario positivo: \" + str(len(vocab_pos)))\n",
=======
>>>>>>> f51ec6b06cf723457f7c3dba65582cec84adc060
    "lexicon = list(vocab_neg.union(vocab_pos))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 4,
>>>>>>> f51ec6b06cf723457f7c3dba65582cec84adc060
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "Cross Validation:\n",
<<<<<<< HEAD
      "Fold  0  - Acuracia:  0.910891089109\n",
      "Fold  1  - Acuracia:  0.841584158416\n",
      "Fold  2  - Acuracia:  0.89\n",
      "Fold  3  - Acuracia:  0.88\n",
      "Fold  4  - Acuracia:  0.9\n",
      "Fold  5  - Acuracia:  0.89\n",
      "Fold  6  - Acuracia:  0.939393939394\n",
      "\n",
      "Accuracia media:  0.89312416956\n",
      "Desvio padrão:  0.0276508060364\n",
=======
      "Fold  0  - Acuracia:  0.816901408451\n",
      "Fold  1  - Acuracia:  0.814285714286\n",
      "Fold  2  - Acuracia:  0.842857142857\n",
      "Fold  3  - Acuracia:  0.814285714286\n",
      "Fold  4  - Acuracia:  0.828571428571\n",
      "Fold  5  - Acuracia:  0.8\n",
      "Fold  6  - Acuracia:  0.8\n",
      "Fold  7  - Acuracia:  0.785714285714\n",
      "Fold  8  - Acuracia:  0.842857142857\n",
      "Fold  9  - Acuracia:  0.8\n",
      "\n",
      "Accuracia media:  0.814547283702\n",
      "Desvio padrão:  0.0180871882636\n",
>>>>>>> f51ec6b06cf723457f7c3dba65582cec84adc060
      "\n",
      "\n",
      "SVM\n",
      "Cross Validation:\n",
<<<<<<< HEAD
      "Fold  0  - Acuracia:  0.861386138614\n",
      "Fold  1  - Acuracia:  0.891089108911\n",
      "Fold  2  - Acuracia:  0.86\n",
      "Fold  3  - Acuracia:  0.92\n",
      "Fold  4  - Acuracia:  0.93\n",
      "Fold  5  - Acuracia:  0.8\n",
      "Fold  6  - Acuracia:  0.878787878788\n",
      "\n",
      "Accuracia media:  0.877323303759\n",
      "Desvio padrão:  0.0402150655682\n",
      "\n",
      "Maxent\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.881188118812\n",
      "Fold  1  - Acuracia:  0.891089108911\n",
      "Fold  2  - Acuracia:  0.89\n",
      "Fold  3  - Acuracia:  0.92\n",
      "Fold  4  - Acuracia:  0.95\n",
      "Fold  5  - Acuracia:  0.83\n",
      "Fold  6  - Acuracia:  0.888888888889\n",
      "\n",
      "Accuracia media:  0.893023730945\n",
      "Desvio padrão:  0.0340505282829\n"
=======
      "Fold  0  - Acuracia:  0.774647887324\n",
      "Fold  1  - Acuracia:  0.842857142857\n",
      "Fold  2  - Acuracia:  0.8\n",
      "Fold  3  - Acuracia:  0.814285714286\n",
      "Fold  4  - Acuracia:  0.871428571429\n",
      "Fold  5  - Acuracia:  0.828571428571\n",
      "Fold  6  - Acuracia:  0.814285714286\n",
      "Fold  7  - Acuracia:  0.714285714286\n",
      "Fold  8  - Acuracia:  0.728571428571\n",
      "Fold  9  - Acuracia:  0.728571428571\n",
      "\n",
      "Accuracia media:  0.791750503018\n",
      "Desvio padrão:  0.0506719580507\n",
      "\n",
      "Maxent\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.816901408451\n",
      "Fold  1  - Acuracia:  0.857142857143\n",
      "Fold  2  - Acuracia:  0.828571428571\n",
      "Fold  3  - Acuracia:  0.842857142857\n",
      "Fold  4  - Acuracia:  0.885714285714\n",
      "Fold  5  - Acuracia:  0.814285714286\n",
      "Fold  6  - Acuracia:  0.814285714286\n",
      "Fold  7  - Acuracia:  0.785714285714\n",
      "Fold  8  - Acuracia:  0.814285714286\n",
      "Fold  9  - Acuracia:  0.8\n",
      "\n",
      "Accuracia media:  0.825975855131\n",
      "Desvio padrão:  0.0276040572433\n"
>>>>>>> f51ec6b06cf723457f7c3dba65582cec84adc060
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = FeatureUnion([\n",
    "#                     (\"bigram\", CountVectorizer(ngram_range=(2,2), stop_words= stopwords_pt, binary= True)),\n",
    "                    (\"lexicon_vector\", CountVectorizer(vocabulary= lexicon, binary = True))\n",
    "                    ])\n",
    "\n",
    "folds = 10;\n",
    "print(\"MultinomialNB\")\n",
    "run_cross_validation(all_data, features, MultinomialNB(), n_folds= folds)\n",
    "\n",
    "print(\"\\n\\nSVM\")\n",
    "run_cross_validation(all_data, features, SVC(C=316), n_folds=folds)\n",
    "\n",
    "print(\"\\nMaxent\")\n",
    "run_cross_validation(all_data, features, LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False), n_folds=folds)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 20,
>>>>>>> f51ec6b06cf723457f7c3dba65582cec84adc060
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Avaliando a incidência dos bigramas\n",
<<<<<<< HEAD
    "cv = CountVectorizer(ngram_range=(1,3))\n",
=======
    "cv = CountVectorizer(ngram_range=(1,2))\n",
>>>>>>> f51ec6b06cf723457f7c3dba65582cec84adc060
    "\n",
    "neg_counts = cv.fit_transform(neg_train[\"texts\"])\n",
    "neg_bigrams = cv.vocabulary_\n",
    "\n",
    "pos_counts = cv.fit_transform(pos_train[\"texts\"])\n",
    "pos_bigrams = cv.vocabulary_\n",
    "\n",
    "sp = np.sum(pos_counts.toarray(), axis=0)\n",
    "sn = np.sum(neg_counts.toarray(), axis=0)\n",
    "\n",
    "pos_bigrams  = {bigram: sp[index] for bigram, index in pos_bigrams.items()}\n",
    "neg_bigrams  = {bigram: sn[index] for bigram, index in neg_bigrams.items()}\n",
    "\n",
    "pos_excl_bigrams = {key:value for key, value in pos_bigrams.items() if key not in neg_bigrams}\n",
    "neg_excl_bigrams = {key:value for key, value in neg_bigrams.items() if key not in pos_bigrams}\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 22,
>>>>>>> f51ec6b06cf723457f7c3dba65582cec84adc060
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "bolsa família     12\n",
       "plebiscito         9\n",
       "primeiro turno     9\n",
       "deus               8\n",
       "direitos           7\n",
       "ibope              7\n",
       "saúde educação     7\n",
       "todas mentiras     7\n",
       "ensino             6\n",
       "internacional      6\n",
       "investir           6\n",
       "américa            5\n",
       "américa latina     5\n",
       "aprovação          5\n",
       "caso eleito        5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
=======
       "educação          20\n",
       "defendeu          11\n",
       "mercado           10\n",
       "saúde             10\n",
       "plebiscito         9\n",
       "acho               7\n",
       "eleito             7\n",
       "bolsa              6\n",
       "cinco              6\n",
       "saúde educação     6\n",
       "segurança          6\n",
       "américa            5\n",
       "américa latina     5\n",
       "avaliação          5\n",
       "cada vez           5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
>>>>>>> f51ec6b06cf723457f7c3dba65582cec84adc060
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Series\n",
    "s = Series(pos_excl_bigrams)\n",
    "s.nlargest(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.87323943662\n",
      "Fold  1  - Acuracia:  0.828571428571\n",
      "Fold  2  - Acuracia:  0.871428571429\n",
      "Fold  3  - Acuracia:  0.885714285714\n",
      "Fold  4  - Acuracia:  0.914285714286\n",
      "Fold  5  - Acuracia:  0.828571428571\n",
      "Fold  6  - Acuracia:  0.857142857143\n",
      "Fold  7  - Acuracia:  0.871428571429\n",
      "Fold  8  - Acuracia:  0.857142857143\n",
      "Fold  9  - Acuracia:  0.814285714286\n",
      "\n",
      "Accuracia media:  0.860181086519\n",
      "Desvio padrão:  0.028506096132\n",
      "\n",
      "\n",
      "SVM\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.633802816901\n",
      "Fold  1  - Acuracia:  0.728571428571\n",
      "Fold  2  - Acuracia:  0.671428571429\n",
      "Fold  3  - Acuracia:  0.714285714286\n",
      "Fold  4  - Acuracia:  0.714285714286\n",
      "Fold  5  - Acuracia:  0.685714285714\n",
      "Fold  6  - Acuracia:  0.671428571429\n",
      "Fold  7  - Acuracia:  0.657142857143\n",
      "Fold  8  - Acuracia:  0.628571428571\n",
      "Fold  9  - Acuracia:  0.7\n",
      "\n",
      "Accuracia media:  0.680523138833\n",
      "Desvio padrão:  0.0325401073154\n",
      "\n",
      "Maxent\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.887323943662\n",
      "Fold  1  - Acuracia:  0.842857142857\n",
      "Fold  2  - Acuracia:  0.828571428571\n",
      "Fold  3  - Acuracia:  0.9\n",
      "Fold  4  - Acuracia:  0.885714285714\n",
      "Fold  5  - Acuracia:  0.814285714286\n",
      "Fold  6  - Acuracia:  0.842857142857\n",
      "Fold  7  - Acuracia:  0.8\n",
      "Fold  8  - Acuracia:  0.871428571429\n",
      "Fold  9  - Acuracia:  0.814285714286\n",
      "\n",
      "Accuracia media:  0.848732394366\n",
      "Desvio padrão:  0.0335021600879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "lexicon_bigrams = list(pos_excl_bigrams.keys()) + list(neg_excl_bigrams.keys())\n",
    "\n",
    "features = FeatureUnion([\n",
    "#                     (\"bigram\", CountVectorizer(ngram_range=(1,1), stop_words= stopwords_pt, vocabulary  = lexicon)),\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_bigrams))\n",
    "                    ])\n",
    "\n",
    "folds = 10;\n",
    "print(\"MultinomialNB\")\n",
    "run_cross_validation(all_data, features, MultinomialNB(), n_folds= folds)\n",
    "\n",
    "print(\"\\n\\nSVM\")\n",
    "run_cross_validation(all_data, features, SVC(C=316), n_folds=folds)\n",
    "\n",
    "print(\"\\nMaxent\")\n",
    "run_cross_validation(all_data, features, LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False), n_folds=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
