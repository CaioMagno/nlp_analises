{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "all_data = get_data_from_db()\n",
    "all_data = all_data[(all_data[\"labels\"]== \"PO\") | (all_data[\"labels\"]== \"NG\")]\n",
    "\n",
    "num_remover = NumRemover()\n",
    "all_data = num_remover.fit_transform(all_data)\n",
    "\n",
    "neg_data = all_data[all_data[\"labels\"] == \"NG\"]\n",
    "pos_data = all_data[all_data[\"labels\"] == \"PO\"]\n",
    "\n",
    "ratio = 0.7\n",
    "neg_train = neg_data.iloc[0:round(ratio*neg_data.shape[0]), :]\n",
    "pos_train = pos_data.iloc[0:round(ratio*pos_data.shape[0]), :]\n",
    "\n",
    "neg_test = neg_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "pos_test = pos_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "\n",
    "stopwords_pt = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão vocabulario negativo: 2630\n",
      "Dimensão vocabulario positivo: 2272\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words= stopwords_pt)\n",
    "\n",
    "cv.fit(neg_train[\"texts\"])\n",
    "vocab_neg = set(cv.vocabulary_.keys())\n",
    "\n",
    "cv.fit(pos_train[\"texts\"])\n",
    "vocab_pos = set(cv.vocabulary_.keys())\n",
    "\n",
    "print(\"Dimensão vocabulario negativo: \" + str(len(vocab_neg)))\n",
    "print(\"Dimensão vocabulario positivo: \" + str(len(vocab_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão:  2734\n"
     ]
    }
   ],
   "source": [
    "# Verificar interseção dos vocabulários\n",
    "intersect = vocab_neg.intersection(vocab_pos)\n",
    "vocab_neg = vocab_neg.difference(intersect)\n",
    "vocab_pos = vocab_pos.difference(intersect)\n",
    "\n",
    "lexicon = list(vocab_neg.union(vocab_pos))\n",
    "print(\"Dimensão: \", len(lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING FEATURE FREQUENCY\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.815955734406\n",
      "Desvio padrão:  0.0208149938779\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.824527162978\n",
      "Desvio padrão:  0.0256455898406\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.781750503018\n",
      "Desvio padrão:  0.0460324225265\n",
      "USING FEATURE PRESENCE\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.813118712274\n",
      "Desvio padrão:  0.01860809145\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.827384305835\n",
      "Desvio padrão:  0.0281973772125\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.790301810865\n",
      "Desvio padrão:  0.0478120064414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(vocabulary= lexicon, binary = False))\n",
    "                    ])\n",
    "folds = 10;\n",
    "evaluate(all_data, features, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12791"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avaliando a incidência dos bigramas\n",
    "cv = CountVectorizer(ngram_range=(1,2), stop_words= stopwords_pt)\n",
    "\n",
    "neg_counts = cv.fit_transform(neg_train[\"texts\"])\n",
    "neg_bigrams = cv.vocabulary_\n",
    "\n",
    "pos_counts = cv.fit_transform(pos_train[\"texts\"])\n",
    "pos_bigrams = cv.vocabulary_\n",
    "\n",
    "sp = np.sum(pos_counts.toarray(), axis=0)\n",
    "sn = np.sum(neg_counts.toarray(), axis=0)\n",
    "\n",
    "pos_bigrams  = {bigram: sp[index] for bigram, index in pos_bigrams.items()}\n",
    "neg_bigrams  = {bigram: sn[index] for bigram, index in neg_bigrams.items()}\n",
    "\n",
    "pos_excl_bigrams = {key:value for key, value in pos_bigrams.items() if key not in neg_bigrams}\n",
    "neg_excl_bigrams = {key:value for key, value in neg_bigrams.items() if key not in pos_bigrams}\n",
    "\n",
    "lexicon_bigrams = list(pos_excl_bigrams.keys()) + list(neg_excl_bigrams.keys())\n",
    "len(lexicon_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "educação          20\n",
       "defendeu          11\n",
       "mercado           10\n",
       "saúde             10\n",
       "plebiscito         9\n",
       "acho               7\n",
       "eleito             7\n",
       "bolsa              6\n",
       "cinco              6\n",
       "saúde educação     6\n",
       "segurança          6\n",
       "américa            5\n",
       "américa latina     5\n",
       "avaliação          5\n",
       "cada vez           5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Series\n",
    "s = Series(pos_excl_bigrams)\n",
    "s.nlargest(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING FEATURE FREQUENCY\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.774587525151\n",
      "Desvio padrão:  0.0409026217338\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.758893360161\n",
      "Desvio padrão:  0.0459261472411\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.714647887324\n",
      "Desvio padrão:  0.0463068580835\n",
      "\n",
      "USING FEATURE PRESENCE\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.774587525151\n",
      "Desvio padrão:  0.0377905610637\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.773138832998\n",
      "Desvio padrão:  0.0522326602087\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.710402414487\n",
      "Desvio padrão:  0.0409928885038\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_bigrams))\n",
    "                    ])\n",
    "\n",
    "print(\"USING FEATURE FREQUENCY\")\n",
    "evaluate(all_data, features, 10)\n",
    "\n",
    "print(\"\\nUSING FEATURE PRESENCE\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_bigrams, binary= True))\n",
    "                    ])\n",
    "evaluate(all_data, features, 10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
