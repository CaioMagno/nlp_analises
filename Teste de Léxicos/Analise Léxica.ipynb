{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "all_data = get_data_from_db()\n",
    "all_data = all_data[(all_data[\"labels\"]== \"PO\") | (all_data[\"labels\"]== \"NG\")]\n",
    "\n",
    "num_remover = NumRemover()\n",
    "all_data = num_remover.fit_transform(all_data)\n",
    "\n",
    "neg_data = all_data[all_data[\"labels\"] == \"NG\"]\n",
    "pos_data = all_data[all_data[\"labels\"] == \"PO\"]\n",
    "\n",
    "ratio = 0.7\n",
    "neg_train = neg_data.iloc[0:round(ratio*neg_data.shape[0]), :]\n",
    "pos_train = pos_data.iloc[0:round(ratio*pos_data.shape[0]), :]\n",
    "\n",
    "neg_test = neg_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "pos_test = pos_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "\n",
    "stopwords_pt = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão vocabulario negativo: 2630\n",
      "Dimensão vocabulario positivo: 2272\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words= stopwords_pt)\n",
    "\n",
    "cv.fit(neg_train[\"texts\"])\n",
    "vocab_neg = set(cv.vocabulary_.keys())\n",
    "\n",
    "cv.fit(pos_train[\"texts\"])\n",
    "vocab_pos = set(cv.vocabulary_.keys())\n",
    "\n",
    "print(\"Dimensão vocabulario negativo: \" + str(len(vocab_neg)))\n",
    "print(\"Dimensão vocabulario positivo: \" + str(len(vocab_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão:  2734\n"
     ]
    }
   ],
   "source": [
    "# Verificar interseção dos vocabulários\n",
    "intersect = vocab_neg.intersection(vocab_pos)\n",
    "vocab_neg = vocab_neg.difference(intersect)\n",
    "vocab_pos = vocab_pos.difference(intersect)\n",
    "\n",
    "lexicon = list(vocab_neg.union(vocab_pos))\n",
    "print(\"Dimensão: \", len(lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING FEATURE FREQUENCY\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.811690140845\n",
      "Desvio padrão:  0.0343164778714\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.803138832998\n",
      "Desvio padrão:  0.0253772032591\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.767464788732\n",
      "Desvio padrão:  0.0362195449787\n",
      "USING FEATURE PRESENCE\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.813118712274\n",
      "Desvio padrão:  0.01860809145\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.827384305835\n",
      "Desvio padrão:  0.0281973772125\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.790301810865\n",
      "Desvio padrão:  0.0478120064414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(strip_accents= \"unicode\", vocabulary= lexicon, binary = False))\n",
    "                    ])\n",
    "\n",
    "print(\"USING FEATURE FREQUENCY\")\n",
    "evaluate(all_data, features, 10)\n",
    "\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(vocabulary= lexicon, binary = True))\n",
    "                    ])\n",
    "print(\"USING FEATURE PRESENCE\")\n",
    "evaluate(all_data, features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13034"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avaliando a incidência dos bigramas\n",
    "cv = CountVectorizer(strip_accents= \"unicode\", ngram_range=(1,2), stop_words= stopwords_pt)\n",
    "\n",
    "neg_counts = cv.fit_transform(neg_train[\"texts\"])\n",
    "neg_bigrams = cv.vocabulary_\n",
    "\n",
    "pos_counts = cv.fit_transform(pos_train[\"texts\"])\n",
    "pos_bigrams = cv.vocabulary_\n",
    "\n",
    "sp = np.sum(pos_counts.toarray(), axis=0)\n",
    "sn = np.sum(neg_counts.toarray(), axis=0)\n",
    "\n",
    "pos_bigrams  = {bigram: sp[index] for bigram, index in pos_bigrams.items()}\n",
    "neg_bigrams  = {bigram: sn[index] for bigram, index in neg_bigrams.items()}\n",
    "\n",
    "pos_excl_bigrams = {key:value for key, value in pos_bigrams.items() if key not in neg_bigrams}\n",
    "neg_excl_bigrams = {key:value for key, value in neg_bigrams.items() if key not in pos_bigrams}\n",
    "\n",
    "lexicon_bigrams = list(pos_excl_bigrams.keys()) + list(neg_excl_bigrams.keys())\n",
    "len(lexicon_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minas energia           15\n",
       "edison                  12\n",
       "edison lobao            12\n",
       "sabia                   12\n",
       "alves                   11\n",
       "sabesp                  11\n",
       "alves pmdb              10\n",
       "camara                  10\n",
       "eduardo alves           10\n",
       "henrique eduardo        10\n",
       "morto                   10\n",
       "nao sabia               10\n",
       "presidente republica    10\n",
       "vaccari                 10\n",
       "calheiros                9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Series\n",
    "s = Series(neg_excl_bigrams)\n",
    "s.nlargest(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING FEATURE FREQUENCY\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.823138832998\n",
      "Desvio padrão:  0.0141450266312\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.85444668008\n",
      "Desvio padrão:  0.034436336099\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.757505030181\n",
      "Desvio padrão:  0.038212831324\n",
      "\n",
      "USING FEATURE PRESENCE\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.830221327968\n",
      "Desvio padrão:  0.0341666014555\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.844486921529\n",
      "Desvio padrão:  0.0252220241979\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.737444668008\n",
      "Desvio padrão:  0.0312582365699\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(strip_accents= \"unicode\", ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_bigrams))\n",
    "                    ])\n",
    "\n",
    "print(\"USING FEATURE FREQUENCY\")\n",
    "evaluate(all_data, features, 10)\n",
    "\n",
    "print(\"\\nUSING FEATURE PRESENCE\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_bigrams, binary= True))\n",
    "                    ])\n",
    "evaluate(all_data, features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING FEATURE FREQUENCY\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.797384305835\n",
      "Desvio padrão:  0.0410000923733\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.838752515091\n",
      "Desvio padrão:  0.0462242890494\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.557766599598\n",
      "Desvio padrão:  0.00187122736419\n",
      "\n",
      "USING FEATURE PRESENCE\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.776016096579\n",
      "Desvio padrão:  0.0443630534937\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.814527162978\n",
      "Desvio padrão:  0.0320598093345\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.557766599598\n",
      "Desvio padrão:  0.00187122736419\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", TfidfVectorizer(strip_accents= \"unicode\", ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_bigrams))\n",
    "                    ])\n",
    "\n",
    "print(\"USING FEATURE FREQUENCY\")\n",
    "evaluate(all_data, features, 10)\n",
    "\n",
    "print(\"\\nUSING FEATURE PRESENCE\")\n",
    "features = FeatureUnion([\n",
    "                    (\"lexicon_vector\", TfidfVectorizer(strip_accents= \"unicode\", ngram_range=(1,1), stop_words=stopwords_pt, vocabulary= lexicon_bigrams, binary= True))\n",
    "                    ])\n",
    "evaluate(all_data, features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation:\n",
      "Accuracia media:  0.85444668008\n",
      "Desvio padrão:  0.034436336099\n",
      "\n",
      "USING FEATURE PRESENCE\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.616257545272\n",
      "Desvio padrão:  0.0462808851999\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.639094567404\n",
      "Desvio padrão:  0.0324848518176\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.639114688129\n",
      "Desvio padrão:  0.0267021343644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest,chi2, mutual_info_classif\n",
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = Pipeline([\n",
    "                    (\"lexicon_vector\", CountVectorizer(strip_accents= \"unicode\", ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_bigrams)),\n",
    "                    (\"feature_selection\", TruncatedSVD(n_components = 6000))\n",
    "                    ])\n",
    "\n",
    "run_cross_validation(all_data, features, LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False), n_folds = 10, shuffle= True)\n",
    "print(\"\\nUSING FEATURE PRESENCE\")\n",
    "features = Pipeline([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_bigrams, binary= True)),\n",
    "                    (\"feature_selection\", VarianceThreshold(threshold = 0.01))\n",
    "                    ])\n",
    "evaluate(all_data, features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bc5b7d2e2c1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                     ])\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrun_cross_validation2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegressionCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/caiomagno/Documentos/Mestrado/Pesquisa/Testes e execução/Analises/Teste de Léxicos/utils.py\u001b[0m in \u001b[0;36mrun_cross_validation2\u001b[0;34m(all_data, features, classifier, n_folds, shuffle)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mtestY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_classifier2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;31m# classifier_models.append(pipeline)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caiomagno/Documentos/Mestrado/Pesquisa/Testes e execução/Analises/Teste de Léxicos/utils.py\u001b[0m in \u001b[0;36mtrain_classifier2\u001b[0;34m(trainX, trainY, testX, testY, classifier)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_classifier2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caiomagno/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n\u001b[0;32m-> 1553\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1554\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caiomagno/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caiomagno/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest,chi2, mutual_info_classif\n",
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = Pipeline([\n",
    "                    (\"lexicon_vector\", CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_bigrams)),\n",
    "                    (\"feature_selection\", TruncatedSVD(n_components = 6000))\n",
    "                    ])\n",
    "\n",
    "run_cross_validation2(all_data, features, LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False), n_folds = 10, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NG</td>\n",
       "      <td>o presidente do pt  rui falcão  acusou na noit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NG</td>\n",
       "      <td>aécio faz coro a denúncias sem provas veicula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NG</td>\n",
       "      <td>segundo falcão  é  cômico   ouvir alguém do ps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NG</td>\n",
       "      <td>ele citou o mensalão mineiro  o esquema de cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NG</td>\n",
       "      <td>para o presidente do pt  aécio deveria se preo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NG</td>\n",
       "      <td>a gravidade das acusações do candidato tucano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NG</td>\n",
       "      <td>o brasil acordou hoje perplexo com as mais gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NG</td>\n",
       "      <td>na campanha de dilma rousseff  a primeira reaç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NG</td>\n",
       "      <td>a preocupação é justificável  quando a polícia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NG</td>\n",
       "      <td>os dois nomes mais citados são o tesoureiro pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NG</td>\n",
       "      <td>preocupação maior está na eventual acusação de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NG</td>\n",
       "      <td>a tensão no governo começou com a publicação e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NG</td>\n",
       "      <td>segundo a folha apurou  dilma ficou  exasperad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NG</td>\n",
       "      <td>do ponto de vista dos estrategistas da campanh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NG</td>\n",
       "      <td>mas parece certo que o caso será alinhavado co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PO</td>\n",
       "      <td>a candidata foi apresentada como alguém que sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PO</td>\n",
       "      <td>marina ainda comentou na tv o resultado do ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>PO</td>\n",
       "      <td>segundo aliados  as acusações de que marina nã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>PO</td>\n",
       "      <td>oficialmente  marina dirá que todas as acusaçõ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>PO</td>\n",
       "      <td>afirmará ainda que está sendo vítima de uma  c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NG</td>\n",
       "      <td>a cúpula do psb mobilizou dirigentes do partid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>PO</td>\n",
       "      <td>marina disse na tv que vai investir o dinheiro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>PO</td>\n",
       "      <td>no meu governo os recursos do pré sal vão ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NG</td>\n",
       "      <td>o candidato à presidência aécio neves  psdb  p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NG</td>\n",
       "      <td>na linha da mensagem em vídeo divulgada pela m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NG</td>\n",
       "      <td>não podemos agora tapar o sol com a peneira  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NG</td>\n",
       "      <td>aécio disse que as denúncias de costa  que ace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NG</td>\n",
       "      <td>o tucano afirmou ainda que o caso mostra que  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>PO</td>\n",
       "      <td>foi uma injeção de ânimo  o presidente lula d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NG</td>\n",
       "      <td>o candidato desconversou sobre a fala do ex pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>NG</td>\n",
       "      <td>a candidata não aceitou a crítica de que entre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>PO</td>\n",
       "      <td>meu caso é muito um exemplo do que estou fala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>PO</td>\n",
       "      <td>questionada sobre as dificuldades de negociar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>PO</td>\n",
       "      <td>a candidata afirmou que o projeto não pode ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>PO</td>\n",
       "      <td>a promessa de que enviaria a proposta ainda ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>NG</td>\n",
       "      <td>dilma voltou a criticar a sua adversária  a ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>NG</td>\n",
       "      <td>é perigosa a vitimização        não estou acu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>NG</td>\n",
       "      <td>sobre a participação da educadora e herdeira d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>NG</td>\n",
       "      <td>questionada sobre ter recebido grandes valores...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>PO</td>\n",
       "      <td>eu recebi para minha campanha  é uma campanha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>PO</td>\n",
       "      <td>mas o que eu sou  sigo a frase  vocês têm de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>PO</td>\n",
       "      <td>a candidata ressaltou depois que apesar de acr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>PO</td>\n",
       "      <td>para dilma  o papa francisco  chefe supremo da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>PO</td>\n",
       "      <td>dilma voltou a se comprometer com a aprovação ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>PO</td>\n",
       "      <td>primeiro eu quero dizer que tenho integral co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>PO</td>\n",
       "      <td>a candidata defendeu ainda a igualdade de dire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>PO</td>\n",
       "      <td>não temos condição de impor obrigatoriedade d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>NG</td>\n",
       "      <td>apesar do leve crescimento  dilma aparece na p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>PO</td>\n",
       "      <td>a nova rodada da pesquisa ibope encomendada pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>PO</td>\n",
       "      <td>na pesquisa ibope do dia  NUM   dilma tinha  N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>PO</td>\n",
       "      <td>em agenda de campanha em fortaleza  a candidat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>PO</td>\n",
       "      <td>para falar do compromisso com programas sociai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>NG</td>\n",
       "      <td>protestos   durante comício realizado no centr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>NG</td>\n",
       "      <td>a presença dos manifestantes provocou um peque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>NG</td>\n",
       "      <td>banco central   a ex senadora aproveitou sua f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>NG</td>\n",
       "      <td>a candidata do psb e seu vice  o deputado fede...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>NG</td>\n",
       "      <td>bancos  as ações das instituições financeiras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>NG</td>\n",
       "      <td>tanto em campanha na rua quanto em seu program...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>NG</td>\n",
       "      <td>a intenção é atingir a candidata marina silva ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>NG</td>\n",
       "      <td>as ações da petrobras fecharam em queda de mai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>701 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     labels                                              texts\n",
       "5        NG  o presidente do pt  rui falcão  acusou na noit...\n",
       "6        NG   aécio faz coro a denúncias sem provas veicula...\n",
       "7        NG  segundo falcão  é  cômico   ouvir alguém do ps...\n",
       "8        NG  ele citou o mensalão mineiro  o esquema de cor...\n",
       "9        NG  para o presidente do pt  aécio deveria se preo...\n",
       "10       NG   a gravidade das acusações do candidato tucano...\n",
       "12       NG   o brasil acordou hoje perplexo com as mais gr...\n",
       "13       NG  na campanha de dilma rousseff  a primeira reaç...\n",
       "14       NG  a preocupação é justificável  quando a polícia...\n",
       "16       NG  os dois nomes mais citados são o tesoureiro pe...\n",
       "18       NG  preocupação maior está na eventual acusação de...\n",
       "19       NG  a tensão no governo começou com a publicação e...\n",
       "20       NG  segundo a folha apurou  dilma ficou  exasperad...\n",
       "21       NG  do ponto de vista dos estrategistas da campanh...\n",
       "23       NG  mas parece certo que o caso será alinhavado co...\n",
       "24       PO  a candidata foi apresentada como alguém que sa...\n",
       "25       PO  marina ainda comentou na tv o resultado do ide...\n",
       "26       PO  segundo aliados  as acusações de que marina nã...\n",
       "27       PO  oficialmente  marina dirá que todas as acusaçõ...\n",
       "28       PO  afirmará ainda que está sendo vítima de uma  c...\n",
       "30       NG  a cúpula do psb mobilizou dirigentes do partid...\n",
       "34       PO  marina disse na tv que vai investir o dinheiro...\n",
       "35       PO   no meu governo os recursos do pré sal vão ser...\n",
       "36       NG  o candidato à presidência aécio neves  psdb  p...\n",
       "37       NG  na linha da mensagem em vídeo divulgada pela m...\n",
       "38       NG   não podemos agora tapar o sol com a peneira  ...\n",
       "39       NG  aécio disse que as denúncias de costa  que ace...\n",
       "40       NG  o tucano afirmou ainda que o caso mostra que  ...\n",
       "43       PO   foi uma injeção de ânimo  o presidente lula d...\n",
       "44       NG  o candidato desconversou sobre a fala do ex pr...\n",
       "...     ...                                                ...\n",
       "1002     NG  a candidata não aceitou a crítica de que entre...\n",
       "1003     PO   meu caso é muito um exemplo do que estou fala...\n",
       "1004     PO  questionada sobre as dificuldades de negociar ...\n",
       "1007     PO  a candidata afirmou que o projeto não pode ser...\n",
       "1008     PO  a promessa de que enviaria a proposta ainda ne...\n",
       "1009     NG  dilma voltou a criticar a sua adversária  a ca...\n",
       "1011     NG   é perigosa a vitimização        não estou acu...\n",
       "1012     NG  sobre a participação da educadora e herdeira d...\n",
       "1013     NG  questionada sobre ter recebido grandes valores...\n",
       "1014     PO   eu recebi para minha campanha  é uma campanha...\n",
       "1016     PO   mas o que eu sou  sigo a frase  vocês têm de ...\n",
       "1017     PO  a candidata ressaltou depois que apesar de acr...\n",
       "1018     PO  para dilma  o papa francisco  chefe supremo da...\n",
       "1019     PO  dilma voltou a se comprometer com a aprovação ...\n",
       "1020     PO   primeiro eu quero dizer que tenho integral co...\n",
       "1021     PO  a candidata defendeu ainda a igualdade de dire...\n",
       "1022     PO   não temos condição de impor obrigatoriedade d...\n",
       "1023     NG  apesar do leve crescimento  dilma aparece na p...\n",
       "1027     PO  a nova rodada da pesquisa ibope encomendada pe...\n",
       "1028     PO  na pesquisa ibope do dia  NUM   dilma tinha  N...\n",
       "1031     PO  em agenda de campanha em fortaleza  a candidat...\n",
       "1032     PO  para falar do compromisso com programas sociai...\n",
       "1033     NG  protestos   durante comício realizado no centr...\n",
       "1034     NG  a presença dos manifestantes provocou um peque...\n",
       "1035     NG  banco central   a ex senadora aproveitou sua f...\n",
       "1036     NG  a candidata do psb e seu vice  o deputado fede...\n",
       "1037     NG  bancos  as ações das instituições financeiras...\n",
       "1038     NG  tanto em campanha na rua quanto em seu program...\n",
       "1039     NG  a intenção é atingir a candidata marina silva ...\n",
       "1040     NG  as ações da petrobras fecharam em queda de mai...\n",
       "\n",
       "[701 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(M)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
