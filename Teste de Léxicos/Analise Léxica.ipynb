{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "all_data = get_data_from_db()\n",
    "all_data = all_data[(all_data[\"labels\"]== \"PO\") | (all_data[\"labels\"]== \"NG\")]\n",
    "\n",
    "num_remover = NumRemover()\n",
    "word_remover = WordRemover(stopwords.words('portuguese'))\n",
    "all_data = num_remover.fit_transform(all_data)\n",
    "all_data = word_remover.fit_transform(all_data)\n",
    "\n",
    "neg_data = all_data[all_data[\"labels\"] == \"NG\"]\n",
    "pos_data = all_data[all_data[\"labels\"] == \"PO\"]\n",
    "\n",
    "ratio = 0.7\n",
    "neg_train = neg_data.iloc[0:round(ratio*neg_data.shape[0]), :]\n",
    "pos_train = pos_data.iloc[0:round(ratio*pos_data.shape[0]), :]\n",
    "\n",
    "neg_test = neg_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "pos_test = pos_data.iloc[round(ratio*neg_data.shape[0]): , :]\n",
    "\n",
    "stopwords_pt = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão vocabulario negativo: 2630\n",
      "Dimensão vocabulario positivo: 2272\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words= stopwords_pt)\n",
    "\n",
    "cv.fit(neg_train[\"texts\"])\n",
    "vocab_neg = set(cv.vocabulary_.keys())\n",
    "\n",
    "cv.fit(pos_train[\"texts\"])\n",
    "vocab_pos = set(cv.vocabulary_.keys())\n",
    "# vocab_neg = set(' '.join(neg_train[\"texts\"].tolist()).split())\n",
    "# vocab_pos = set(' '.join(pos_train[\"texts\"].tolist()).split())\n",
    "\n",
    "print(\"Dimensão vocabulario negativo: \" + str(len(vocab_neg)))\n",
    "print(\"Dimensão vocabulario positivo: \" + str(len(vocab_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interseção:  1084\n"
     ]
    }
   ],
   "source": [
    "# Verificar interseção dos vocabulários\n",
    "intersect = vocab_neg.intersection(vocab_pos)\n",
    "vocab_neg = vocab_neg.difference(intersect)\n",
    "vocab_pos = vocab_pos.difference(intersect)\n",
    "\n",
    "\n",
    "print(\"Interseção: \", len(intersect))\n",
    "\n",
    "lexicon = list(vocab_neg.union(vocab_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.815955734406\n",
      "Desvio padrão:  0.0208149938779\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.824527162978\n",
      "Desvio padrão:  0.0256455898406\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.781750503018\n",
      "Desvio padrão:  0.0460324225265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = FeatureUnion([\n",
    "#                     (\"bigram\", CountVectorizer(ngram_range=(2,2), stop_words= stopwords_pt, binary= True)),\n",
    "                    (\"lexicon_vector\", CountVectorizer(vocabulary= lexicon, binary = False))\n",
    "                    ])\n",
    "folds = 10;\n",
    "evaluate(all_data, features, folds)\n",
    "\n",
    "# print(\"MultinomialNB\")\n",
    "# run_cross_validation(all_data, features, MultinomialNB(), n_folds= folds)\n",
    "\n",
    "# print(\"\\n\\nSVM\")\n",
    "# run_cross_validation(all_data, features, SVC(C=316), n_folds=folds)\n",
    "\n",
    "# print(\"\\nMaxent\")\n",
    "# run_cross_validation(all_data, features, LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False), n_folds=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15430"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avaliando a incidência dos bigramas\n",
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "neg_counts = cv.fit_transform(neg_train[\"texts\"])\n",
    "neg_bigrams = cv.vocabulary_\n",
    "\n",
    "pos_counts = cv.fit_transform(pos_train[\"texts\"])\n",
    "pos_bigrams = cv.vocabulary_\n",
    "\n",
    "sp = np.sum(pos_counts.toarray(), axis=0)\n",
    "sn = np.sum(neg_counts.toarray(), axis=0)\n",
    "\n",
    "pos_bigrams  = {bigram: sp[index] for bigram, index in pos_bigrams.items()}\n",
    "neg_bigrams  = {bigram: sn[index] for bigram, index in neg_bigrams.items()}\n",
    "\n",
    "pos_excl_bigrams = {key:value for key, value in pos_bigrams.items() if key not in neg_bigrams}\n",
    "neg_excl_bigrams = {key:value for key, value in neg_bigrams.items() if key not in pos_bigrams}\n",
    "\n",
    "lexicon_bigrams = list(pos_excl_bigrams.keys()) + list(neg_excl_bigrams.keys())\n",
    "len(lexicon_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minas energia       15\n",
       "edison              12\n",
       "edison lobão        12\n",
       "sabia               12\n",
       "alves               11\n",
       "sabesp              11\n",
       "alves pmdb          10\n",
       "câmara              10\n",
       "eduardo alves       10\n",
       "henrique eduardo    10\n",
       "morto               10\n",
       "não sabia           10\n",
       "pelo ex             10\n",
       "vaccari             10\n",
       "calheiros            9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Series\n",
    "s = Series(neg_excl_bigrams)\n",
    "s.nlargest(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "Cross Validation:\n",
      "Accuracia media:  0.851649899396\n",
      "Desvio padrão:  0.0293590327385\n",
      "\n",
      "\n",
      "SVM\n",
      "Cross Validation:\n",
      "Accuracia media:  0.69615694165\n",
      "Desvio padrão:  0.0337314147193\n",
      "\n",
      "Maxent\n",
      "Cross Validation:\n",
      "Accuracia media:  0.837303822938\n",
      "Desvio padrão:  0.0391693402297\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = FeatureUnion([\n",
    "#                     (\"bigram\", CountVectorizer(ngram_range=(1,1), stop_words= stopwords_pt, vocabulary  = lexicon)),\n",
    "                    (\"lexicon_vector\", CountVectorizer(binary= True, ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_bigrams))\n",
    "                    ])\n",
    "\n",
    "folds = 10;\n",
    "print(\"MultinomialNB\")\n",
    "run_cross_validation(all_data, features, MultinomialNB(), n_folds= folds)\n",
    "\n",
    "print(\"\\n\\nSVM\")\n",
    "run_cross_validation(all_data, features, SVC(C=316), n_folds=folds)\n",
    "\n",
    "print(\"\\nMaxent\")\n",
    "run_cross_validation(all_data, features, LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False), n_folds=folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Usando o PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation:\n",
      "Accuracia media:  0.848752515091\n",
      "Desvio padrão:  0.0358056246919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pca = PCA()\n",
    "pca = TruncatedSVD(n_components=500)\n",
    "cv  = CountVectorizer(ngram_range=(1,2), stop_words=stopwords_pt, vocabulary= lexicon_bigrams)\n",
    "\n",
    "t_data = cv.fit_transform(all_data[\"texts\"])\n",
    "t_data = pca.fit_transform(t_data.toarray())\n",
    "\n",
    "labels = np.array([])\n",
    "for label in all_data[\"labels\"]:\n",
    "    if label == 'PO':\n",
    "        labels = np.append(labels, [1])\n",
    "    elif label == \"NG\":\n",
    "        labels = np.append(labels, [-1])\n",
    "    elif label == \"NE\":\n",
    "        labels = np.append(labels, [0])\n",
    "\n",
    "labels.shape\n",
    "\n",
    "classifier = LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False)\n",
    "# classifier = MultinomialNB()\n",
    "\n",
    "print(\"Cross Validation:\")\n",
    "accuracy_average = np.array([])\n",
    "sKFold = StratifiedKFold(n_splits= 10, shuffle= True, random_state= True)\n",
    "for index, (train, test) in enumerate(sKFold.split(t_data, labels)):\n",
    "    # Treinando um modelo Naive Bayes\n",
    "    train_data = t_data[train]\n",
    "    test_data = t_data[test]\n",
    "\n",
    "    classifier.fit(train_data, labels[train])\n",
    "    predictions = classifier.predict(test_data)\n",
    "\n",
    "    accuracy = accuracy_score(labels[test], predictions, normalize=True)\n",
    "    # classifier_models.append(pipeline)\n",
    "\n",
    "    accuracy_average = np.append(accuracy_average, accuracy)\n",
    "    #print(\"Fold \", index, \" - Acuracia: \", accuracy)\n",
    "\n",
    "print(\"Accuracia media: \", accuracy_average.mean())\n",
    "print(\"Desvio padrão: \", accuracy_average.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consegui um resultado equivalente com uma dimensionalidade menor"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
