{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "all_data = get_data_from_db()\n",
    "all_data = all_data[(all_data[\"labels\"]== \"PO\") | (all_data[\"labels\"]== \"NG\")]\n",
    "\n",
    "num_remover = NumRemover()\n",
    "word_remover = WordRemover(stopwords.words('portuguese'))\n",
    "all_data = num_remover.fit_transform(all_data)\n",
    "all_data = word_remover.fit_transform(all_data)\n",
    "\n",
    "neg_data = all_data[all_data[\"labels\"] == \"NG\"]\n",
    "pos_data = all_data[all_data[\"labels\"] == \"PO\"]\n",
    "\n",
    "neg_train = neg_data.iloc[0:round(0.3*neg_data.shape[0]), :]\n",
    "pos_train = pos_data.iloc[0:round(0.3*pos_data.shape[0]), :]\n",
    "\n",
    "neg_test = neg_data.iloc[round(0.3*neg_data.shape[0]): , :]\n",
    "pos_test = pos_data.iloc[round(0.3*neg_data.shape[0]): , :]\n",
    "\n",
    "stopwords_pt = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão vocabulario negativo: 1435\n",
      "Dimensão vocabulario positivo: 1362\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab_neg = set(' '.join(neg_train[\"texts\"].tolist()).split())\n",
    "vocab_pos = set(' '.join(pos_train[\"texts\"].tolist()).split())\n",
    "\n",
    "print(\"Dimensão vocabulario negativo: \" + str(len(vocab_neg)))\n",
    "print(\"Dimensão vocabulario positivo: \" + str(len(vocab_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Verificar interseção dos vocabulários\n",
    "intersect = vocab_neg.intersection(vocab_pos)\n",
    "vocab_neg = vocab_neg.difference(intersect)\n",
    "vocab_pos = vocab_pos.difference(intersect)\n",
    "\n",
    "lexicon = list(vocab_neg.union(vocab_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.802816901408\n",
      "Fold  1  - Acuracia:  0.771428571429\n",
      "Fold  2  - Acuracia:  0.771428571429\n",
      "Fold  3  - Acuracia:  0.871428571429\n",
      "Fold  4  - Acuracia:  0.828571428571\n",
      "Fold  5  - Acuracia:  0.814285714286\n",
      "Fold  6  - Acuracia:  0.757142857143\n",
      "Fold  7  - Acuracia:  0.742857142857\n",
      "Fold  8  - Acuracia:  0.7\n",
      "Fold  9  - Acuracia:  0.657142857143\n",
      "\n",
      "Accuracia media:  0.771710261569\n",
      "Desvio padrão:  0.0593886757314\n",
      "\n",
      "\n",
      "SVM\n",
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.774647887324\n",
      "Fold  1  - Acuracia:  0.828571428571\n",
      "Fold  2  - Acuracia:  0.742857142857\n",
      "Fold  3  - Acuracia:  0.771428571429\n",
      "Fold  4  - Acuracia:  0.828571428571\n",
      "Fold  5  - Acuracia:  0.771428571429\n",
      "Fold  6  - Acuracia:  0.742857142857\n",
      "Fold  7  - Acuracia:  0.642857142857\n",
      "Fold  8  - Acuracia:  0.657142857143\n",
      "Fold  9  - Acuracia:  0.6\n",
      "\n",
      "Accuracia media:  0.736036217304\n",
      "Desvio padrão:  0.0739101033911\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "features = FeatureUnion([\n",
    "                    (\"bigram\", CountVectorizer(ngram_range=(2,2), stop_words= stopwords_pt, binary= True)),\n",
    "                    (\"lexicon_vector\", CountVectorizer(vocabulary= lexicon))\n",
    "                    ])\n",
    "\n",
    "print(\"MultinomialNB\")\n",
    "run_cross_validation(all_data, features, MultinomialNB())\n",
    "\n",
    "print(\"\\n\\nSVM\")\n",
    "run_cross_validation(all_data, features, SVC(C=316))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
