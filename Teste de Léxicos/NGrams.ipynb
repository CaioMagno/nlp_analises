{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidade\n",
      "Unigram, 2 labels:  4977\n",
      "Unigram, 3 labels:  5938\n",
      "Bigram, 2 labels:  15525\n",
      "Bigram, 3 labels:  21065\n",
      "Unibigram, 2 labels:  20502\n",
      "Unibigram, 3 labels:  27003\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "numRemover = NumRemover()\n",
    "\n",
    "all_data3 = get_data_from_db()\n",
    "all_data3 = numRemover.fit_transform(all_data3)\n",
    "all_data2 = all_data3[(all_data3[\"labels\"]== \"PO\") | (all_data3[\"labels\"]== \"NG\")]\n",
    "\n",
    "unigramVectorizer = CountVectorizer(ngram_range=(1,1), stop_words= stopwords.words('portuguese'))\n",
    "bigramVectorizer = CountVectorizer(ngram_range=(2,2), stop_words= stopwords.words('portuguese'))\n",
    "unibigramVectorizer = CountVectorizer(ngram_range=(1,2), stop_words= stopwords.words('portuguese'))\n",
    "\n",
    "print(\"Dimensionalidade\")\n",
    "print(\"Unigram, 2 labels: \", unigramVectorizer.fit_transform(all_data2['texts']).shape[1])\n",
    "print(\"Unigram, 3 labels: \", unigramVectorizer.fit_transform(all_data3['texts']).shape[1])\n",
    "print(\"Bigram, 2 labels: \", bigramVectorizer.fit_transform(all_data2['texts']).shape[1])\n",
    "print(\"Bigram, 3 labels: \", bigramVectorizer.fit_transform(all_data3['texts']).shape[1])\n",
    "print(\"Unibigram, 2 labels: \", unibigramVectorizer.fit_transform(all_data2['texts']).shape[1])\n",
    "print(\"Unibigram, 3 labels: \", unibigramVectorizer.fit_transform(all_data3['texts']).shape[1])\n",
    "\n",
    "def evaluate(data, vectorizer, n_folds):\n",
    "    #print(\"Naive Bayes---------------------------------\")\n",
    "    #run_cross_validation(data, vectorizer, MultinomialNB(), n_folds = n_folds)\n",
    "    print(\"\\nMaxEnt--------------------------------------\")\n",
    "    run_cross_validation(data, vectorizer, LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False), n_folds = n_folds)\n",
    "    print(\"\\nSVM-----------------------------------------\")\n",
    "    run_cross_validation(data, vectorizer, SVC(C=316), n_folds = n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************\n",
      "Unigrams, Positive and Negative Texts\n",
      "********************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.740402414487\n",
      "Desvio padrão:  0.0590152747254\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.714728370221\n",
      "Desvio padrão:  0.0520688270576\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.689054325956\n",
      "Desvio padrão:  0.041457013093\n",
      "********************************************\n",
      "********************************************\n",
      "Unigrams, Positive, Negative and Neutral Texts\n",
      "********************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.591074020319\n",
      "Desvio padrão:  0.0318120872586\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.561284470247\n",
      "Desvio padrão:  0.037788302971\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.551669085631\n",
      "Desvio padrão:  0.0497701263446\n",
      "********************************************\n"
     ]
    }
   ],
   "source": [
    "# Learning algorithms\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(\"********************************************\")\n",
    "print(\"Unigrams, Positive and Negative Texts\")\n",
    "print(\"********************************************\")\n",
    "evaluate(all_data2, unigramVectorizer, 10)\n",
    "print(\"********************************************\")\n",
    "\n",
    "print(\"********************************************\")\n",
    "print(\"Unigrams, Positive, Negative and Neutral Texts\")\n",
    "print(\"********************************************\")\n",
    "evaluate(all_data3, unigramVectorizer, 10)\n",
    "print(\"********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************\n",
      "Bigrams, Positive and Negative Texts\n",
      "********************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.734486921529\n",
      "Desvio padrão:  0.0562139478682\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.708913480885\n",
      "Desvio padrão:  0.054973048298\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.706116700201\n",
      "Desvio padrão:  0.0516679829356\n",
      "********************************************\n",
      "\n",
      "********************************************\n",
      "Bigrams, Positive, Negative and Neutral Texts\n",
      "********************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.554698838897\n",
      "Desvio padrão:  0.02979894565\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.527703193033\n",
      "Desvio padrão:  0.0354927271111\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.509542815675\n",
      "Desvio padrão:  0.0438056805642\n",
      "********************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"********************************************\")\n",
    "print(\"Bigrams, Positive and Negative Texts\")\n",
    "print(\"********************************************\")\n",
    "evaluate(all_data2, bigramVectorizer, 10)\n",
    "print(\"********************************************\\n\")\n",
    "\n",
    "print(\"********************************************\")\n",
    "print(\"Bigrams, Positive, Negative and Neutral Texts\")\n",
    "print(\"********************************************\")\n",
    "evaluate(all_data3, bigramVectorizer, 10)\n",
    "print(\"********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************\n",
      "Unigrams and Bigrams, Positive and Negative Texts\n",
      "********************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.758913480885\n",
      "Desvio padrão:  0.0467087610498\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.723319919517\n",
      "Desvio padrão:  0.0519344270977\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.70754527163\n",
      "Desvio padrão:  0.0449155907986\n",
      "********************************************\n",
      "\n",
      "********************************************\n",
      "Unigrams and Bigrams, Positive, Negative and Neutral Texts\n",
      "********************************************\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.593087808418\n",
      "Desvio padrão:  0.0333112829391\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.569920174165\n",
      "Desvio padrão:  0.0315720973231\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.571861393324\n",
      "Desvio padrão:  0.0353460329846\n",
      "********************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"********************************************\")\n",
    "print(\"Unigrams and Bigrams, Positive and Negative Texts\")\n",
    "print(\"********************************************\")\n",
    "evaluate(all_data2, unibigramVectorizer, 10)\n",
    "print(\"********************************************\\n\")\n",
    "\n",
    "print(\"********************************************\")\n",
    "print(\"Unigrams and Bigrams, Positive, Negative and Neutral Texts\")\n",
    "print(\"********************************************\")\n",
    "evaluate(all_data3, unibigramVectorizer, 10)\n",
    "print(\"********************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Usando o PCA com 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(701,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pca = PCA()\n",
    "pca = TruncatedSVD(n_components=701)\n",
    "cv  = CountVectorizer(stop_words= stopwords.words('portuguese'))\n",
    "\n",
    "t_data = cv.fit_transform(all_data2[\"texts\"])\n",
    "t_data = pca.fit_transform(t_data.toarray())\n",
    "\n",
    "labels = np.array([])\n",
    "for label in all_data2[\"labels\"]:\n",
    "    if label == 'PO':\n",
    "        labels = np.append(labels, [1])\n",
    "    elif label == \"NG\":\n",
    "        labels = np.append(labels, [-1])\n",
    "    elif label == \"NE\":\n",
    "        pass\n",
    "#         labels.append(0)\n",
    "\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation:\n",
      "Accuracia media:  0.711871227364\n",
      "Desvio padrão:  0.0512239816422\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False)\n",
    "# classifier = MultinomialNB()\n",
    "\n",
    "print(\"Cross Validation:\")\n",
    "accuracy_average = np.array([])\n",
    "sKFold = StratifiedKFold(n_splits= 10, shuffle= True, random_state= True)\n",
    "for index, (train, test) in enumerate(sKFold.split(t_data, labels)):\n",
    "    # Treinando um modelo Naive Bayes\n",
    "    train_data = t_data[train]\n",
    "    test_data = t_data[test]\n",
    "\n",
    "    classifier.fit(train_data, labels[train])\n",
    "    predictions = classifier.predict(test_data)\n",
    "\n",
    "    accuracy = accuracy_score(labels[test], predictions, normalize=True)\n",
    "    # classifier_models.append(pipeline)\n",
    "\n",
    "    accuracy_average = np.append(accuracy_average, accuracy)\n",
    "    #print(\"Fold \", index, \" - Acuracia: \", accuracy)\n",
    "\n",
    "print(\"Accuracia media: \", accuracy_average.mean())\n",
    "print(\"Desvio padrão: \", accuracy_average.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o pca com 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation:\n",
      "Accuracia media:  0.5662191582\n",
      "Desvio padrão:  0.0447801669792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pca = PCA()\n",
    "pca = TruncatedSVD(n_components=701)\n",
    "cv  = CountVectorizer(stop_words= stopwords.words('portuguese'))\n",
    "\n",
    "t_data = cv.fit_transform(all_data3[\"texts\"])\n",
    "t_data = pca.fit_transform(t_data.toarray())\n",
    "\n",
    "labels = np.array([])\n",
    "for label in all_data3[\"labels\"]:\n",
    "    if label == 'PO':\n",
    "        labels = np.append(labels, [1])\n",
    "    elif label == \"NG\":\n",
    "        labels = np.append(labels, [-1])\n",
    "    elif label == \"NE\":\n",
    "        labels = np.append(labels, [0])\n",
    "\n",
    "labels.shape\n",
    "\n",
    "classifier = LogisticRegressionCV(fit_intercept=False, penalty= 'l2', dual= False)\n",
    "# classifier = MultinomialNB()\n",
    "\n",
    "print(\"Cross Validation:\")\n",
    "accuracy_average = np.array([])\n",
    "sKFold = StratifiedKFold(n_splits= 10, shuffle= True, random_state= True)\n",
    "for index, (train, test) in enumerate(sKFold.split(t_data, labels)):\n",
    "    # Treinando um modelo Naive Bayes\n",
    "    train_data = t_data[train]\n",
    "    test_data = t_data[test]\n",
    "\n",
    "    classifier.fit(train_data, labels[train])\n",
    "    predictions = classifier.predict(test_data)\n",
    "\n",
    "    accuracy = accuracy_score(labels[test], predictions, normalize=True)\n",
    "    # classifier_models.append(pipeline)\n",
    "\n",
    "    accuracy_average = np.append(accuracy_average, accuracy)\n",
    "    #print(\"Fold \", index, \" - Acuracia: \", accuracy)\n",
    "\n",
    "print(\"Accuracia media: \", accuracy_average.mean())\n",
    "print(\"Desvio padrão: \", accuracy_average.std())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
