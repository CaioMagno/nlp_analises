{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aqui usamos somente o conjunto de treinamento para selecionar as melhores features com Mutual Info\n",
    "\n",
    "from utils import  *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "\n",
    "# 1) Separar o corpus por classes\n",
    "numRemover = NumRemover()\n",
    "all_data = numRemover.fit_transform(get_data_from_db())\n",
    "seed = int(random.uniform(0, 100))\n",
    "train_ratio = 0.7\n",
    "train, test, ytrain, ytest = train_test_split(all_data, all_data[\"labels\"], train_size = train_ratio, stratify = all_data[\"labels\"], random_state = seed)\n",
    "\n",
    "# 2) Criar um vetorizer para unigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), stop_words= stopwords.words(\"portuguese\"), strip_accents= \"unicode\")\n",
    "\n",
    "# 3) Fazer o fit para todo o corpus\n",
    "X = vectorizer.fit_transform(all_data[\"texts\"])\n",
    "\n",
    "# 4) Aplicar o metodo de seleção\n",
    "dataX = SelectKBest(mutual_info_classif, k = 8000).fit_transform(X.toarray(), all_data[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation:\n",
      "Fold  0  - Acuracia:  0.556603773585\n",
      "Fold  1  - Acuracia:  0.567307692308\n",
      "Fold  2  - Acuracia:  0.605769230769\n",
      "Fold  3  - Acuracia:  0.576923076923\n",
      "Fold  4  - Acuracia:  0.634615384615\n",
      "Fold  5  - Acuracia:  0.653846153846\n",
      "Fold  6  - Acuracia:  0.567307692308\n",
      "Fold  7  - Acuracia:  0.557692307692\n",
      "Fold  8  - Acuracia:  0.615384615385\n",
      "Fold  9  - Acuracia:  0.557692307692\n",
      "Accuracia media:  0.589314223512\n",
      "Desvio padrão:  0.0337030826868\n"
     ]
    }
   ],
   "source": [
    "sKFold = StratifiedKFold(n_splits= 10, shuffle= True, random_state= True)\n",
    "print(\"Cross Validation:\")\n",
    "accuracy_average = np.array([])\n",
    "dataY = all_data[\"labels\"]\n",
    "classifier = MultinomialNB()\n",
    "for index, (train, test) in enumerate(sKFold.split(dataX, dataY)):\n",
    "    # Treinando um modelo Naive Bayes\n",
    "    trainX = dataX[train]\n",
    "    testX = dataX[test]\n",
    "\n",
    "    trainY = dataY[train]\n",
    "    testY = dataY[test]\n",
    "\n",
    "    accuracy = train_classifier2(trainX, trainY, testX, testY, classifier)\n",
    "    # classifier_models.append(pipeline)\n",
    "\n",
    "    accuracy_average = np.append(accuracy_average, accuracy)\n",
    "    print(\"Fold \", index, \" - Acuracia: \", accuracy)\n",
    "\n",
    "print(\"Accuracia media: \", accuracy_average.mean())\n",
    "print(\"Desvio padrão: \", accuracy_average.std())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
