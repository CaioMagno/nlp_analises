{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proporcional Difference\n",
    "\n",
    "Teste do método de seleção de features Proporcional Difference proposto em http://crpit.com/confpapers/CRPITV87Simeon.pdf\n",
    "\n",
    "Este método foi testado contra outros, inclusive o SentiWordNet, em:\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.471.5694&rep=rep1&type=pdf#page=77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidade:  23444\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.742761248186\n",
      "Desvio padrão:  0.0402812737308\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.897296806967\n",
      "Desvio padrão:  0.0220098213888\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.494375907112\n",
      "Desvio padrão:  0.0349591032269\n"
     ]
    }
   ],
   "source": [
    "# Aqui estou usando todo o corpus para selecionar as melhores features.\n",
    "\n",
    "from utils import  *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from pandas import DataFrame\n",
    "\n",
    "# 1) Separar o corpus por classes\n",
    "numRemover = NumRemover()\n",
    "all_data = numRemover.fit_transform(get_data_from_db())\n",
    "pos_data = numRemover.fit_transform(get_data_from_db(sentiment=\"PO\"))[\"texts\"]\n",
    "neg_data = numRemover.fit_transform(get_data_from_db(sentiment=\"NG\"))[\"texts\"]\n",
    "neu_data = numRemover.fit_transform(get_data_from_db(sentiment=\"NE\"))[\"texts\"]\n",
    "\n",
    "# 2) Criar um vetorizer para unigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), stop_words= stopwords.words(\"portuguese\"), strip_accents= \"unicode\")\n",
    "\n",
    "# 3) Fazer o fit para todo o corpus\n",
    "vectorizer.fit(all_data[\"texts\"])\n",
    "\n",
    "# 4) Pegar todo o vocabulário do corpus\n",
    "vocab = list(vectorizer.vocabulary_.keys())\n",
    "vocab_indexed = {value: key for key, value in vectorizer.vocabulary_.items()}\n",
    "\n",
    "# 5) Obter as matrizes das classes\n",
    "pos_matrix = vectorizer.transform(pos_data)\n",
    "neg_matrix = vectorizer.transform(neg_data)\n",
    "neu_matrix = vectorizer.transform(neu_data)\n",
    "\n",
    "# 6) Obter o vetor de soma das frquencias das palavras nas classes\n",
    "pos_sum = pos_matrix.sum(axis=0)\n",
    "neg_sum = neg_matrix.sum(axis=0)\n",
    "neu_sum = neu_matrix.sum(axis=0)\n",
    "\n",
    "# 7) Construir um dicionario de frequencias para cada classe e calcular o PD de cada uma\n",
    "# função que calcula o PD\n",
    "pd = lambda c1, c2, c3: max([(c1 - c2 - c3)/(c1 + c2 + c3), (c2 - c1 - c3)/(c1 + c2 + c3), (c3 - c2 - c1)/(c1 + c2 + c3)])\n",
    "freq_dict = [ {\"PO\": pos_sum[0,index], \"NG\": neg_sum[0,index], \"NE\": neu_sum[0,index], \"PD\": pd(pos_sum[0,index], neg_sum[0,index], neu_sum[0,index])} for index in vocab_indexed.keys()]\n",
    "freq_df = DataFrame(data = freq_dict, index= vocab_indexed.values())\n",
    "freq_df = freq_df[[\"PO\", \"NG\", \"NE\", \"PD\"]] # Reordenando as colunas do data frame\n",
    "\n",
    "# 8) Selecionar as melhores features\n",
    "threshold = 1\n",
    "selected = freq_df[freq_df[\"PD\"] == threshold].index.values\n",
    "selected = list(selected)\n",
    "print(\"Dimensionalidade: \", len(selected))\n",
    "\n",
    "# 9) Avaliar o desempenho dos classificadores utilizando as features selecionadas\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1,2), stop_words= stopwords.words(\"portuguese\"), strip_accents= \"unicode\", vocabulary= selected)\n",
    "evaluate(all_data, unigram_vectorizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidade:  17728\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.635359216255\n",
      "Desvio padrão:  0.0341172871526\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.732220609579\n",
      "Desvio padrão:  0.0476956566556\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.593196661829\n",
      "Desvio padrão:  0.0314476741683\n"
     ]
    }
   ],
   "source": [
    "# Aqui usamos somente o conjunto de treinamento para selecionar as melhores features com PD\n",
    "\n",
    "from utils import  *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "\n",
    "# 1) Separar o corpus por classes\n",
    "numRemover = NumRemover()\n",
    "all_data = numRemover.fit_transform(get_data_from_db())\n",
    "seed = int(random.uniform(0, 100))\n",
    "train_ratio = 0.7\n",
    "train, test, ytrain, ytest = train_test_split(all_data, all_data[\"labels\"], train_size = train_ratio, stratify = all_data[\"labels\"], random_state = seed)\n",
    "\n",
    "pos_data = train[train[\"labels\"] == \"PO\"][\"texts\"]\n",
    "neg_data = train[train[\"labels\"] == \"NG\"][\"texts\"]\n",
    "neu_data = train[train[\"labels\"] == \"NE\"][\"texts\"]\n",
    "\n",
    "# 2) Criar um vetorizer para unigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), stop_words= stopwords.words(\"portuguese\"), strip_accents= \"unicode\")\n",
    "\n",
    "# 3) Fazer o fit para todo o corpus\n",
    "vectorizer.fit(train[\"texts\"])\n",
    "\n",
    "# 4) Pegar todo o vocabulário do corpus\n",
    "vocab = list(vectorizer.vocabulary_.keys())\n",
    "vocab_indexed = {value: key for key, value in vectorizer.vocabulary_.items()}\n",
    "\n",
    "# 5) Obter as matrizes das classes\n",
    "pos_matrix = vectorizer.transform(pos_data)\n",
    "neg_matrix = vectorizer.transform(neg_data)\n",
    "neu_matrix = vectorizer.transform(neu_data)\n",
    "\n",
    "# 6) Obter o vetor de soma das frquencias das palavras nas classes\n",
    "pos_sum = pos_matrix.sum(axis=0)\n",
    "neg_sum = neg_matrix.sum(axis=0)\n",
    "neu_sum = neu_matrix.sum(axis=0)\n",
    "\n",
    "# 7) Construir um dicionario de frequencias para cada classe e calcular o PD de cada uma\n",
    "# função que calcula o PD\n",
    "pd = lambda c1, c2, c3: max([(c1 - c2 - c3)/(c1 + c2 + c3), (c2 - c1 - c3)/(c1 + c2 + c3), (c3 - c2 - c1)/(c1 + c2 + c3)])\n",
    "freq_dict = [ {\"PO\": pos_sum[0,index], \"NG\": neg_sum[0,index], \"NE\": neu_sum[0,index], \"PD\": pd(pos_sum[0,index], neg_sum[0,index], neu_sum[0,index])} for index in vocab_indexed.keys()]\n",
    "freq_df = DataFrame(data = freq_dict, index= vocab_indexed.values())\n",
    "freq_df = freq_df[[\"PO\", \"NG\", \"NE\", \"PD\"]] # Reordenando as colunas do data frame\n",
    "\n",
    "# 8) Selecionar as melhores features\n",
    "threshold = 1\n",
    "selected = freq_df[freq_df[\"PD\"] >= threshold].index.values\n",
    "selected = list(selected)\n",
    "print(\"Dimensionalidade: \", len(selected))\n",
    "\n",
    "# 9) Avaliar o desempenho dos classificadores utilizando as features selecionadas\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1,2), stop_words= stopwords.words(\"portuguese\"), strip_accents= \"unicode\", vocabulary= selected)\n",
    "evaluate(all_data, unigram_vectorizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidade:  21455\n",
      "Naive Bayes---------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.721661828737\n",
      "Desvio padrão:  0.0390942898891\n",
      "\n",
      "MaxEnt--------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.835885341074\n",
      "Desvio padrão:  0.0252679329833\n",
      "\n",
      "SVM-----------------------------------------\n",
      "Cross Validation:\n",
      "Accuracia media:  0.522224238026\n",
      "Desvio padrão:  0.0434539102682\n"
     ]
    }
   ],
   "source": [
    "# Aqui usamos somente o conjunto de treinamento para selecionar as melhores features com PD\n",
    "\n",
    "from utils import  *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "\n",
    "# 1) Separar o corpus por classes\n",
    "numRemover = NumRemover()\n",
    "all_data = numRemover.fit_transform(get_data_from_db())\n",
    "seed = int(random.uniform(0, 100))\n",
    "train_ratio = 0.9\n",
    "train, test, ytrain, ytest = train_test_split(all_data, all_data[\"labels\"], train_size = train_ratio, stratify = all_data[\"labels\"], random_state = seed)\n",
    "\n",
    "pos_data = train[train[\"labels\"] == \"PO\"][\"texts\"]\n",
    "neg_data = train[train[\"labels\"] == \"NG\"][\"texts\"]\n",
    "neu_data = train[train[\"labels\"] == \"NE\"][\"texts\"]\n",
    "\n",
    "# 2) Criar um vetorizer para unigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), stop_words= stopwords.words(\"portuguese\"), strip_accents= \"unicode\")\n",
    "\n",
    "# 3) Fazer o fit para todo o corpus\n",
    "vectorizer.fit(train[\"texts\"])\n",
    "\n",
    "# 4) Pegar todo o vocabulário do corpus\n",
    "vocab = list(vectorizer.vocabulary_.keys())\n",
    "vocab_indexed = {value: key for key, value in vectorizer.vocabulary_.items()}\n",
    "\n",
    "# 5) Obter as matrizes das classes\n",
    "pos_matrix = vectorizer.transform(pos_data)\n",
    "neg_matrix = vectorizer.transform(neg_data)\n",
    "neu_matrix = vectorizer.transform(neu_data)\n",
    "\n",
    "# 6) Obter o vetor de soma das frquencias das palavras nas classes\n",
    "pos_sum = pos_matrix.sum(axis=0)\n",
    "neg_sum = neg_matrix.sum(axis=0)\n",
    "neu_sum = neu_matrix.sum(axis=0)\n",
    "\n",
    "# 7) Construir um dicionario de frequencias para cada classe e calcular o PD de cada uma\n",
    "# função que calcula o PD\n",
    "pd = lambda c1, c2, c3: max([(c1 - c2 - c3)/(c1 + c2 + c3), (c2 - c1 - c3)/(c1 + c2 + c3), (c3 - c2 - c1)/(c1 + c2 + c3)])\n",
    "freq_dict = [ {\"PO\": pos_sum[0,index], \"NG\": neg_sum[0,index], \"NE\": neu_sum[0,index], \"PD\": pd(pos_sum[0,index], neg_sum[0,index], neu_sum[0,index])} for index in vocab_indexed.keys()]\n",
    "freq_df = DataFrame(data = freq_dict, index= vocab_indexed.values())\n",
    "freq_df = freq_df[[\"PO\", \"NG\", \"NE\", \"PD\"]] # Reordenando as colunas do data frame\n",
    "\n",
    "# 8) Selecionar as melhores features\n",
    "threshold = 1\n",
    "selected = freq_df[freq_df[\"PD\"] >= threshold].index.values\n",
    "selected = list(selected)\n",
    "print(\"Dimensionalidade: \", len(selected))\n",
    "\n",
    "# 9) Avaliar o desempenho dos classificadores utilizando as features selecionadas\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1,2), stop_words= stopwords.words(\"portuguese\"), strip_accents= \"unicode\", vocabulary= selected)\n",
    "evaluate(all_data, unigram_vectorizer, 10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
